{
          "cells": [
                    {
                              "cell_type": "markdown",
                              "id": "SSdMAJbEN8dX",
                              "metadata": {
                                        "id": "SSdMAJbEN8dX"
                              },
                              "source": [
                                        "# Gemini API Basics: Free Key, First Calls, and Prompting\n",
                                        "\n",
                                        "## What you will learn\n",
                                        "- What a closed-model API is and why teams use it.\n",
                                        "- How to use a free Gemini API key in Colab.\n",
                                        "- How prompt changes affect model outputs.\n",
                                        "\n",
                                        "Expected runtime: 30-45 minutes\n",
                                        "Expected cost: Free tier if usage stays within quota.\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "qvdmzvtdN8dZ",
                              "metadata": {
                                        "id": "qvdmzvtdN8dZ"
                              },
                              "source": [
                                        "## Setup notes\n",
                                        "1. Store your key as a Colab Secret named GOOGLE_API_KEY.\n",
                                        "2. If key setup fails, you can still complete analysis tasks using reference outputs.\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "TTKeMeCQN8dc",
                              "metadata": {
                                        "colab": {
                                                  "base_uri": "https://localhost:8080/"
                                        },
                                        "id": "TTKeMeCQN8dc",
                                        "outputId": "47496360-011c-4325-fa89-1a824c3dbfc6"
                              },
                              "outputs": [],
                              "source": [
                                        "%pip -q install -U google-genai\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "hTrJhCHiN8dd",
                              "metadata": {
                                        "id": "hTrJhCHiN8dd"
                              },
                              "outputs": [],
                              "source": [
                                        "import os\n",
                                        "import time\n",
                                        "import pandas as pd\n",
                                        "\n",
                                        "GEMINI_AVAILABLE = False\n",
                                        "GEMINI_ERROR = None\n",
                                        "\n",
                                        "try:\n",
                                        "    from google import genai\n",
                                        "\n",
                                        "    api_key = os.getenv('GOOGLE_API_KEY')\n",
                                        "    if not api_key:\n",
                                        "        try:\n",
                                        "            from google.colab import userdata\n",
                                        "            api_key = userdata.get('GOOGLE_API_KEY')\n",
                                        "        except Exception:\n",
                                        "            api_key = None\n",
                                        "\n",
                                        "    if not api_key:\n",
                                        "        raise ValueError('GOOGLE_API_KEY not found. Set env var or Colab secret GOOGLE_API_KEY.')\n",
                                        "\n",
                                        "    client = genai.Client(api_key=api_key)\n",
                                        "    GEMINI_AVAILABLE = True\n",
                                        "except Exception as e:\n",
                                        "    GEMINI_ERROR = str(e)\n",
                                        "    print('Gemini is not available in this runtime.')\n",
                                        "    print('Reason:', GEMINI_ERROR)\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "-kDyJwMmN8de",
                              "metadata": {
                                        "colab": {
                                                  "base_uri": "https://localhost:8080/"
                                        },
                                        "id": "-kDyJwMmN8de",
                                        "outputId": "279a0418-6777-4c0d-c52a-e9470e9fca5b"
                              },
                              "outputs": [],
                              "source": [
                                        "MODEL_ID = 'gemini-2.5-flash'\n",
                                        "print('Model:', MODEL_ID)\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "JR4hhwv2N8df",
                              "metadata": {
                                        "id": "JR4hhwv2N8df"
                              },
                              "outputs": [],
                              "source": [
                                        "def run_gemini(prompt: str, system_instruction=None):\n",
                                        "    if not GEMINI_AVAILABLE:\n",
                                        "        return {\n",
                                        "            'ok': False,\n",
                                        "            'model': MODEL_ID,\n",
                                        "            'prompt': prompt,\n",
                                        "            'system_instruction': system_instruction,\n",
                                        "            'text': None,\n",
                                        "            'latency_s': None,\n",
                                        "            'tokens_in': None,\n",
                                        "            'error': GEMINI_ERROR,\n",
                                        "        }\n",
                                        "\n",
                                        "    config = {'system_instruction': system_instruction} if system_instruction else None\n",
                                        "\n",
                                        "    try:\n",
                                        "        start = time.perf_counter()\n",
                                        "        response = client.models.generate_content(model=MODEL_ID, contents=prompt, config=config)\n",
                                        "        latency = time.perf_counter() - start\n",
                                        "\n",
                                        "        token_info = client.models.count_tokens(model=MODEL_ID, contents=prompt)\n",
                                        "        tokens_in = getattr(token_info, 'total_tokens', None)\n",
                                        "\n",
                                        "        return {\n",
                                        "            'ok': True,\n",
                                        "            'model': MODEL_ID,\n",
                                        "            'prompt': prompt,\n",
                                        "            'system_instruction': system_instruction,\n",
                                        "            'text': response.text,\n",
                                        "            'latency_s': round(latency, 2),\n",
                                        "            'tokens_in': tokens_in,\n",
                                        "            'error': None,\n",
                                        "        }\n",
                                        "    except Exception as e:\n",
                                        "        return {\n",
                                        "            'ok': False,\n",
                                        "            'model': MODEL_ID,\n",
                                        "            'prompt': prompt,\n",
                                        "            'system_instruction': system_instruction,\n",
                                        "            'text': None,\n",
                                        "            'latency_s': None,\n",
                                        "            'tokens_in': None,\n",
                                        "            'error': str(e),\n",
                                        "        }\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "1a219189",
                              "metadata": {
                                        "colab": {
                                                  "base_uri": "https://localhost:8080/"
                                        },
                                        "id": "1a219189",
                                        "outputId": "293332e7-cea4-484e-d5a6-1df9d3236bd2"
                              },
                              "outputs": [],
                              "source": [
                                        "# First basic call (no system prompt)\n",
                                        "first_prompt = 'Explain GPU and VRAM to a high-school student in 5 bullet points.'\n",
                                        "first_result = run_gemini(first_prompt)\n",
                                        "\n",
                                        "print('--- First Gemini Call ---')\n",
                                        "print('Prompt:', first_prompt)\n",
                                        "print('Error:', first_result['error'])\n",
                                        "print('Output:\\n')\n",
                                        "print(first_result['text'])\n",
                                        "\n",
                                        "# Prompt for system prompt comparison\n",
                                        "comparison_prompt = 'Explain the concept of large language models (LLMs).'\n",
                                        "system_prompt_1 = 'You are a helpful assistant that provides concise answers.'\n",
                                        "system_prompt_2 = 'You are a helpful assistant that provides detailed explanations, especially for technical terms.'\n",
                                        "\n",
                                        "print('\\n--- System Prompt Comparison Setup ---')\n",
                                        "print(f'User prompt: {comparison_prompt}')\n",
                                        "print(f'System Prompt 1: {system_prompt_1}')\n",
                                        "print(f'System Prompt 2: {system_prompt_2}')\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "18f120de",
                              "metadata": {
                                        "colab": {
                                                  "base_uri": "https://localhost:8080/",
                                                  "height": 1000
                                        },
                                        "id": "18f120de",
                                        "outputId": "2cedeccc-e64b-4e62-bba0-a2ca186a0770"
                              },
                              "outputs": [],
                              "source": [
                                        "baseline_result = run_gemini(comparison_prompt)\n",
                                        "result_sp1 = run_gemini(comparison_prompt, system_instruction=system_prompt_1)\n",
                                        "result_sp2 = run_gemini(comparison_prompt, system_instruction=system_prompt_2)\n",
                                        "\n",
                                        "print('\\n--- Output: No System Prompt ---')\n",
                                        "print(baseline_result['text'])\n",
                                        "\n",
                                        "print('\\n--- Output: System Prompt 1 (Concise) ---')\n",
                                        "print(result_sp1['text'])\n",
                                        "\n",
                                        "print('\\n--- Output: System Prompt 2 (Detailed) ---')\n",
                                        "print(result_sp2['text'])\n",
                                        "\n",
                                        "comparison_df = pd.DataFrame([\n",
                                        "    {'prompt_type': 'No System Prompt', 'latency_s': baseline_result['latency_s'], 'tokens_in': baseline_result['tokens_in'], 'error': baseline_result['error'], 'text': baseline_result['text']},\n",
                                        "    {'prompt_type': 'System Prompt 1: Concise', 'latency_s': result_sp1['latency_s'], 'tokens_in': result_sp1['tokens_in'], 'error': result_sp1['error'], 'text': result_sp1['text']},\n",
                                        "    {'prompt_type': 'System Prompt 2: Detailed', 'latency_s': result_sp2['latency_s'], 'tokens_in': result_sp2['tokens_in'], 'error': result_sp2['error'], 'text': result_sp2['text']}\n",
                                        "])\n",
                                        "\n",
                                        "comparison_df\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "jUtCZuERN8df",
                              "metadata": {
                                        "id": "jUtCZuERN8df"
                              },
                              "outputs": [],
                              "source": [
                                        "# Additional prompt examples for basic API calling\n",
                                        "prompts = [\n",
                                        "    'What is the difference between open-weight and closed models? Keep it simple.',\n",
                                        "    'Compare open-weight and closed models for a small startup. Include privacy, cost, and control in a table.'\n",
                                        "]\n",
                                        "\n",
                                        "rows = [run_gemini(p) for p in prompts]\n",
                                        "results_df = pd.DataFrame(rows)\n",
                                        "results_df[['prompt', 'latency_s', 'tokens_in', 'error', 'text']]\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "5UKBi0jpN8df",
                              "metadata": {
                                        "id": "5UKBi0jpN8df"
                              },
                              "outputs": [],
                              "source": [
                                        "# Optional: quick side-by-side excerpt view for teaching\n",
                                        "preview_df = comparison_df.copy()\n",
                                        "preview_df['text_preview'] = preview_df['text'].fillna('').str.slice(0, 240)\n",
                                        "preview_df[['prompt_type', 'latency_s', 'tokens_in', 'error', 'text_preview']]\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "NpjJOIQAN8dg",
                              "metadata": {
                                        "id": "NpjJOIQAN8dg"
                              },
                              "source": [
                                        "## Checkpoint\n",
                                        "- Rewrite a weak prompt into a strong prompt for the same task.\n",
                                        "- Note 2 output differences you observe.\n",
                                        "\n",
                                        "## Reflection\n",
                                        "- When would a closed API model be better than running local models?\n",
                                        "\n",
                                        "## Troubleshooting\n",
                                        "- If key lookup fails, re-open Colab Secrets and verify GOOGLE_API_KEY.\n",
                                        "- Free-tier quota limits can cause temporary request failures.\n"
                              ]
                    }
          ],
          "metadata": {
                    "colab": {
                              "gpuType": "T4",
                              "provenance": []
                    },
                    "kernelspec": {
                              "display_name": "Python 3 (ipykernel)",
                              "language": "python",
                              "name": "python3"
                    },
                    "language_info": {
                              "codemirror_mode": {
                                        "name": "ipython",
                                        "version": 3
                              },
                              "file_extension": ".py",
                              "mimetype": "text/x-python",
                              "name": "python",
                              "nbconvert_exporter": "python",
                              "pygments_lexer": "ipython3",
                              "version": "3.12.12"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 5
}
