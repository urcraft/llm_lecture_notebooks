{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculate VRAM requirements for open source models\n",
        "---\n",
        "\n",
        "This notebook shows how to run [`hf-mem`](https://github.com/alvarobartt/hf-mem) from a Google Colab runtime using `uvx`.\n",
        "\n",
        "`hf-mem` estimates inference memory requirements for models hosted on Hugging Face Hub.\n",
        "\n",
        "## Before you proceed\n",
        "---\n",
        "\n",
        "1. In Colab, go to **Runtime > Change runtime type** and select **T4 GPU** (or better).\n",
        "2. Then run **Runtime > Run all**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n",
        "---\n",
        "\n",
        "We install `uv` and verify that `uvx` is available in this runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "82 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "downloading uv 0.10.4 x86_64-unknown-linux-gnu\n",
            "no checksums to verify\n",
            "installing to /usr/local/bin\n",
            "  uv\n",
            "  uvx\n",
            "everything's installed!\n",
            "uv 0.10.4\n",
            "uvx 0.10.4\n"
          ]
        }
      ],
      "source": [
        "!apt -qq update\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "!/usr/local/bin/uv --version\n",
        "!/usr/local/bin/uvx --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run hf-mem on Popular Hugging Face LLMs\n",
        "---\n",
        "\n",
        "Each command runs separately (no loop).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K\u001b[2mInstalled \u001b[1m11 packages\u001b[0m \u001b[2min 12ms\u001b[0m\u001b[0m                               \u001b[0m         \n",
            "\u001b[38;2;244;183;63m┌┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┐\u001b[0m\n",
            "\u001b[38;2;244;183;63m├┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│            INFERENCE MEMORY ESTIMATE FOR            │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│      https://hf.co/MiniMaxAI/MiniMax-M2 @ main      │\u001b[0m\n",
            "\u001b[38;2;244;183;63m├────────────────┬────────────────────────────────────┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ TOTAL MEMORY   │ 214.32 GB (228.70B PARAMS)         │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ REQUIREMENTS   │ ██████████████████████████████████ │\u001b[0m\n",
            "\u001b[38;2;244;183;63m├────────────────┼────────────────────────────────────┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ F32            │ 0.23 / 214.32 GB                   │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ 62.65M PARAMS  │ ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ │\u001b[0m\n",
            "\u001b[38;2;244;183;63m├────────────────┼────────────────────────────────────┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ F8_E4M3        │ 211.79 / 214.32 GB                 │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ 227.41B PARAMS │ ██████████████████████████████████ │\u001b[0m\n",
            "\u001b[38;2;244;183;63m├────────────────┼────────────────────────────────────┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ BF16           │ 2.29 / 214.32 GB                   │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ 1.23B PARAMS   │ ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ │\u001b[0m\n",
            "\u001b[38;2;244;183;63m└────────────────┴────────────────────────────────────┘\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!/usr/local/bin/uvx hf-mem --model-id MiniMaxAI/MiniMax-M2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2m                                                                              \u001b[0m\u001b[38;2;244;183;63m┌┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┐\u001b[0m\n",
            "\u001b[38;2;244;183;63m├┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│            INFERENCE MEMORY ESTIMATE FOR             │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│          https://hf.co/zai-org/GLM-5 @ main          │\u001b[0m\n",
            "\u001b[38;2;244;183;63m├────────────────┬─────────────────────────────────────┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ TOTAL MEMORY   │ 1404.18 GB (753.86B PARAMS)         │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ REQUIREMENTS   │ ███████████████████████████████████ │\u001b[0m\n",
            "\u001b[38;2;244;183;63m├────────────────┼─────────────────────────────────────┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ BF16           │ 1404.18 / 1404.18 GB                │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ 753.86B PARAMS │ ███████████████████████████████████ │\u001b[0m\n",
            "\u001b[38;2;244;183;63m├────────────────┼─────────────────────────────────────┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ F32            │ 0.00 / 1404.18 GB                   │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ 19.46K PARAMS  │ ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ │\u001b[0m\n",
            "\u001b[38;2;244;183;63m└────────────────┴─────────────────────────────────────┘\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!/usr/local/bin/uvx hf-mem --model-id zai-org/GLM-5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2m                                                                              \u001b[0m\u001b[38;2;244;183;63m┌┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┬┐\u001b[0m\n",
            "\u001b[38;2;244;183;63m├┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┴┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│          INFERENCE MEMORY ESTIMATE FOR          │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│       https://hf.co/Qwen/Qwen3-4B @ main        │\u001b[0m\n",
            "\u001b[38;2;244;183;63m├────────────────┬────────────────────────────────┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ TOTAL MEMORY   │ 7.49 GB (4.02B PARAMS)         │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ REQUIREMENTS   │ ██████████████████████████████ │\u001b[0m\n",
            "\u001b[38;2;244;183;63m├────────────────┼────────────────────────────────┤\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ BF16           │ 7.49 / 7.49 GB                 │\u001b[0m\n",
            "\u001b[38;2;244;183;63m│ 4.02B PARAMS   │ ██████████████████████████████ │\u001b[0m\n",
            "\u001b[38;2;244;183;63m└────────────────┴────────────────────────────────┘\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!/usr/local/bin/uvx hf-mem --model-id Qwen/Qwen3-4B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Feb 21 11:39:27 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "---\n",
        "\n",
        "- If a model is gated/private, authenticate in Colab first (e.g., `huggingface-cli login`).\n",
        "- `hf-mem` is experimental and may change across releases.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "HF_MEM_Colab_UVX.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
