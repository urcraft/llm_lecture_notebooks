{
    "cells":  [
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "\u003ca target=\"_blank\" href=\"https://github.com/urcraft/llm_lecture_notebooks/blob/main/04_Open_vs_Closed_Models_Gemini_vs_Local_Comparison.ipynb\"\u003e",
                                     "  \u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e",
                                     "\u003c/a\u003e"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "_d4F0mNomWCQ",
                      "metadata":  {
                                       "id":  "_d4F0mNomWCQ"
                                   },
                      "source":  [
                                     "# Open vs Closed Models: Gemini vs Local Comparison\n",
                                     "\n",
                                     "## What you will learn\n",
                                     "- How to compare model outputs with a simple rubric.\n",
                                     "- Tradeoffs: quality, latency, cost proxy, and privacy.\n",
                                     "\n",
                                     "Expected runtime: 30-45 minutes\n",
                                     "Expected cost: Gemini free tier plus local runtime resources.\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "850693b4",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "850693b4",
                                       "outputId":  "42745ed1-6ce6-48ef-8b13-e8b3ce2334c0"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "%pip -q install -U google-genai"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "0b13db40",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "0b13db40",
                                       "outputId":  "a4d2f133-0f47-423e-d9f7-4d2ca7b01540"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "!sudo apt -qq update\n",
                                     "!sudo apt -qq install -y pciutils zstd\n",
                                     "!curl -fsSL https://ollama.com/install.sh | sh\n",
                                     "\n",
                                     "import time\n",
                                     "import subprocess\n",
                                     "import pandas as pd\n",
                                     "\n",
                                     "LOCAL_MODEL = \u0027qwen3:4b\u0027\n",
                                     "MODEL_ID = \u0027gemini-3-flash-preview\u0027\n",
                                     "\n",
                                     "def start_ollama_service():\n",
                                     "    subprocess.Popen([\u0027ollama\u0027, \u0027serve\u0027])\n",
                                     "    time.sleep(5)\n",
                                     "\n",
                                     "def pull_local_model(model_name: str):\n",
                                     "    try:\n",
                                     "        subprocess.run([\u0027ollama\u0027, \u0027pull\u0027, model_name], check=True)\n",
                                     "        return True\n",
                                     "    except Exception as e:\n",
                                     "        print(\u0027Could not pull local model:\u0027, e)\n",
                                     "        return False\n",
                                     "\n",
                                     "start_ollama_service()\n",
                                     "LOCAL_AVAILABLE = pull_local_model(LOCAL_MODEL)\n",
                                     "print(\u0027LOCAL_AVAILABLE =\u0027, LOCAL_AVAILABLE)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "d3196528",
                      "metadata":  {
                                       "id":  "d3196528"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "GEMINI_AVAILABLE = False\n",
                                     "GEMINI_ERROR = None\n",
                                     "\n",
                                     "try:\n",
                                     "    from google import genai\n",
                                     "    import os\n",
                                     "    HARDCODED_GOOGLE_API_KEY = None\n",
                                     "    api_key = HARDCODED_GOOGLE_API_KEY or os.getenv(\u0027GOOGLE_API_KEY\u0027)\n",
                                     "    if not api_key:\n",
                                     "        from google.colab import userdata\n",
                                     "        api_key = userdata.get(\u0027GOOGLE_API_KEY\u0027)\n",
                                     "    if not api_key:\n",
                                     "        raise ValueError(\u0027Set HARDCODED_GOOGLE_API_KEY or GOOGLE_API_KEY\u0027)\n",
                                     "    client = genai.Client(api_key=api_key)\n",
                                     "    GEMINI_AVAILABLE = True\n",
                                     "except Exception as e:\n",
                                     "    GEMINI_ERROR = str(e)\n",
                                     "    print(\u0027Gemini unavailable:\u0027, GEMINI_ERROR)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "38e528b4",
                      "metadata":  {
                                       "id":  "38e528b4"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "def run_local(prompt: str):\n",
                                     "    start = time.perf_counter()\n",
                                     "    if not LOCAL_AVAILABLE:\n",
                                     "        return {\u0027ok\u0027: False, \u0027model\u0027: LOCAL_MODEL, \u0027output\u0027: None, \u0027latency_s\u0027: None, \u0027error\u0027: \u0027Local model unavailable\u0027}\n",
                                     "    try:\n",
                                     "        proc = subprocess.run([\u0027ollama\u0027, \u0027run\u0027, LOCAL_MODEL, prompt], capture_output=True, text=True, check=True)\n",
                                     "        return {\u0027ok\u0027: True, \u0027model\u0027: LOCAL_MODEL, \u0027output\u0027: proc.stdout.strip(), \u0027latency_s\u0027: round(time.perf_counter() - start, 2), \u0027error\u0027: None}\n",
                                     "    except Exception as e:\n",
                                     "        return {\u0027ok\u0027: False, \u0027model\u0027: LOCAL_MODEL, \u0027output\u0027: None, \u0027latency_s\u0027: None, \u0027error\u0027: str(e)}\n",
                                     "\n",
                                     "\n",
                                     "def run_gemini(prompt: str):\n",
                                     "    start = time.perf_counter()\n",
                                     "    if not GEMINI_AVAILABLE:\n",
                                     "        return {\u0027ok\u0027: False, \u0027model\u0027: MODEL_ID, \u0027output\u0027: None, \u0027latency_s\u0027: None, \u0027error\u0027: GEMINI_ERROR}\n",
                                     "    try:\n",
                                     "        response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
                                     "        return {\u0027ok\u0027: True, \u0027model\u0027: MODEL_ID, \u0027output\u0027: response.text, \u0027latency_s\u0027: round(time.perf_counter() - start, 2), \u0027error\u0027: None}\n",
                                     "    except Exception as e:\n",
                                     "        return {\u0027ok\u0027: False, \u0027model\u0027: MODEL_ID, \u0027output\u0027: None, \u0027latency_s\u0027: None, \u0027error\u0027: str(e)}\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "9b7ff61a",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/",
                                                     "height":  445
                                                 },
                                       "id":  "9b7ff61a",
                                       "outputId":  "c2d3a8fd-5059-42a3-ea1f-804c195fba66"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "TASKS = [\n",
                                     "    \u0027A Danish mid-sized B2B company wants to deploy a customer-support AI assistant. Give a one-paragraph recommendation on whether to start with an open-weight model or a closed model, and why.\u0027,\n",
                                     "    \u0027You are advising an enterprise buyer in Denmark. In one paragraph, compare GPU vs CPU choices for running AI inference in production, focusing on cost, latency, and scalability tradeoffs.\u0027,\n",
                                     "    \u0027In one paragraph, propose a simple pilot plan for evaluating two LLM vendors for internal knowledge search in a regulated enterprise setting (success metrics, timeline, and key risks).\u0027\n",
                                     "]\n",
                                     "\n",
                                     "local_rows = []\n",
                                     "for task in TASKS:\n",
                                     "    local_res = run_local(task)\n",
                                     "    local_rows.append({\n",
                                     "        \u0027task\u0027: task,\n",
                                     "        \u0027model\u0027: local_res[\u0027model\u0027],\n",
                                     "        \u0027output\u0027: local_res[\u0027output\u0027],\n",
                                     "        \u0027latency_s\u0027: local_res[\u0027latency_s\u0027],\n",
                                     "        \u0027error\u0027: local_res[\u0027error\u0027]\n",
                                     "    })\n",
                                     "\n",
                                     "local_df = pd.DataFrame(local_rows)\n",
                                     "print(f\u0027Local run completed for {len(local_df)} tasks.\u0027)\n",
                                     "local_df\n",
                                     "\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "3860e837",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/",
                                                     "height":  795
                                                 },
                                       "id":  "3860e837",
                                       "outputId":  "10c08e11-1175-4b0a-f7a2-31f09ae510ad"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "if \u0027TASKS\u0027 not in globals() or \u0027local_df\u0027 not in globals():\n",
                                     "    raise RuntimeError(\u0027Run the local-task cell first to create TASKS and local_df.\u0027)\n",
                                     "\n",
                                     "gemini_rows = []\n",
                                     "failed_gemini_rows = []\n",
                                     "\n",
                                     "for task in TASKS:\n",
                                     "    gem_res = run_gemini(task)\n",
                                     "    if gem_res[\u0027ok\u0027]:\n",
                                     "        gemini_rows.append({\n",
                                     "            \u0027task\u0027: task,\n",
                                     "            \u0027model\u0027: gem_res[\u0027model\u0027],\n",
                                     "            \u0027output\u0027: gem_res[\u0027output\u0027],\n",
                                     "            \u0027latency_s\u0027: gem_res[\u0027latency_s\u0027],\n",
                                     "            \u0027error\u0027: gem_res[\u0027error\u0027]\n",
                                     "        })\n",
                                     "    else:\n",
                                     "        failed_gemini_rows.append({\n",
                                     "            \u0027task\u0027: task,\n",
                                     "            \u0027model\u0027: MODEL_ID,\n",
                                     "            \u0027error\u0027: gem_res[\u0027error\u0027]\n",
                                     "        })\n",
                                     "\n",
                                     "gemini_df = pd.DataFrame(gemini_rows)\n",
                                     "failed_gemini_df = pd.DataFrame(failed_gemini_rows)\n",
                                     "\n",
                                     "if not failed_gemini_df.empty:\n",
                                     "    print(\u0027Some Gemini calls failed. Re-run this Gemini cell to retry failed tasks.\u0027)\n",
                                     "    failed_gemini_df\n",
                                     "else:\n",
                                     "    print(\u0027Gemini run completed for all tasks.\u0027)\n",
                                     "\n",
                                     "comparison_df = pd.concat([local_df, gemini_df], ignore_index=True)\n",
                                     "print(f\u0027Rows included in comparison_df: {len(comparison_df)} (Local: {len(local_df)}, Gemini successful: {len(gemini_df)})\u0027)\n",
                                     "comparison_df\n",
                                     "\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "comparison_df[\u0027quality_score_1_to_5\u0027] = \u0027\u0027\n",
                                     "comparison_df[\u0027factuality_score_1_to_5\u0027] = \u0027\u0027\n",
                                     "comparison_df[\u0027notes\u0027] = \u0027\u0027\n",
                                     "\n",
                                     "export_path = \u0027comparison_df_student_ratings.xlsx\u0027\n",
                                     "comparison_df.to_excel(export_path, index=False)\n",
                                     "print(f\u0027Saved {export_path}\u0027)\n",
                                     "\n",
                                     "if \u0027failed_gemini_df\u0027 in globals() and not failed_gemini_df.empty:\n",
                                     "    print(\u0027Note: Failed Gemini rows were excluded from the exported XLSX. Re-run the Gemini cell if needed.\u0027)\n",
                                     "\n",
                                     "try:\n",
                                     "    from google.colab import files\n",
                                     "    files.download(export_path)\n",
                                     "except Exception:\n",
                                     "    print(\u0027If not running in Colab, download the file from the notebook working directory.\u0027)\n",
                                     "\n",
                                     "comparison_df\n",
                                     "\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "w3Xoud20mWCa",
                      "metadata":  {
                                       "id":  "w3Xoud20mWCa"
                                   },
                      "source":  [
                                     "## Checkpoint\n",
                                     "- Fill in the scoring columns for each output.\n",
                                     "- Identify one task where local wins and one where Gemini wins.\n",
                                     "\n",
                                     "## Reflection\n",
                                     "- Which model would you choose for: (a) sensitive data, (b) fastest setup, (c) predictable cost?\n",
                                     "\n",
                                     "## Troubleshooting\n",
                                     "- If local model pull fails, verify runtime and available memory.\n",
                                     "- If Gemini fails, re-run the Gemini cell; failed Gemini rows are not exported to XLSX.\n",
                                     "\n"
                                 ]
                  }
              ],
    "metadata":  {
                     "accelerator":  "GPU",
                     "colab":  {
                                   "gpuType":  "T4",
                                   "provenance":  [

                                                  ]
                               },
                     "kernelspec":  {
                                        "display_name":  "Python 3 (ipykernel)",
                                        "language":  "python",
                                        "name":  "python3"
                                    },
                     "language_info":  {
                                           "codemirror_mode":  {
                                                                   "name":  "ipython",
                                                                   "version":  3
                                                               },
                                           "file_extension":  ".py",
                                           "mimetype":  "text/x-python",
                                           "name":  "python",
                                           "nbconvert_exporter":  "python",
                                           "pygments_lexer":  "ipython3",
                                           "version":  "3.12.12"
                                       }
                 },
    "nbformat":  4,
    "nbformat_minor":  5
}
