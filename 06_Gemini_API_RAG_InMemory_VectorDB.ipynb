{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhHQ0-11__n6"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/urcraft/llm_lecture_notebooks/blob/main/06_Gemini_API_RAG_InMemory_VectorDB.ipynb\">   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a>"
      ],
      "id": "ZhHQ0-11__n6"
    },
    {
      "cell_type": "markdown",
      "id": "CIVlEH-2aIDv",
      "metadata": {
        "id": "CIVlEH-2aIDv"
      },
      "source": [
        "# Gemini API + In-Memory Vector DB (RAG)\n",
        "\n",
        "## What you will build\n",
        "- A minimal Retrieval-Augmented Generation (RAG) pipeline.\n",
        "- Gemini embeddings for chunked documents.\n",
        "- Chroma in-memory vector database for retrieval.\n",
        "- Gemini answer generation grounded in retrieved context.\n",
        "\n",
        "Expected runtime: 10-20 minutes\n",
        "Expected cost: Zero for Gemini Free tier API usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pc5eANXNaIDx",
      "metadata": {
        "id": "pc5eANXNaIDx"
      },
      "source": [
        "## Online References Used\n",
        "- Gemini embeddings API reference: https://ai.google.dev/api/embeddings\n",
        "- Gemini text generation guide: https://ai.google.dev/gemini-api/docs/system-instructions\n",
        "- Chroma in-memory client docs: https://docs.trychroma.com/docs/run-chroma/clients\n",
        "- Chroma Python reference: https://docs.trychroma.com/reference/python\n",
        "- Gemini cookbook repository: https://github.com/google-gemini/cookbook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "pVRI_403aIDx",
      "metadata": {
        "id": "pVRI_403aIDx",
        "outputId": "3f4e3de2-ef26-4945-8c8d-cab1c47a4a2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m728.8/728.8 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
            "grain 0.2.15 requires protobuf>=5.28.3, but you have protobuf 4.25.8 which is incompatible.\n",
            "google-adk 1.25.0 requires opentelemetry-api<1.40.0,>=1.36.0, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
            "google-adk 1.25.0 requires opentelemetry-exporter-otlp-proto-http>=1.36.0, but you have opentelemetry-exporter-otlp-proto-http 1.22.0 which is incompatible.\n",
            "google-adk 1.25.0 requires opentelemetry-sdk<1.40.0,>=1.36.0, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
            "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
            "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
            "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
            "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
            "google-cloud-pubsub 2.35.0 requires opentelemetry-api>=1.27.0, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
            "google-cloud-pubsub 2.35.0 requires opentelemetry-sdk>=1.27.0, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
            "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
            "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.15.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Force-install compatible versions for the Colab environment\n",
        "%pip install -q -U google-genai chromadb \\\n",
        "    \"pandas==2.2.2\" \\\n",
        "    \"requests==2.32.4\" \\\n",
        "    \"opentelemetry-sdk==1.22.0\" \\\n",
        "    \"opentelemetry-exporter-otlp-proto-http==1.22.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "GrWsPjfKaIDy",
      "metadata": {
        "id": "GrWsPjfKaIDy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import textwrap\n",
        "import requests\n",
        "import pandas as pd\n",
        "import chromadb\n",
        "\n",
        "from google import genai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "q8EwMjpzaIDz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8EwMjpzaIDz",
        "outputId": "8c1ae0d8-8a66-4b67-9de5-1c4c0a3194ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini client ready\n",
            "Embedding model: gemini-embedding-001\n",
            "Generation model: gemini-3-flash-preview\n"
          ]
        }
      ],
      "source": [
        "# Set your API key in environment variable GOOGLE_API_KEY before running.\n",
        "# In Colab, you can also use a secret named GOOGLE_API_KEY.\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if not api_key:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "    except Exception:\n",
        "        api_key = None\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"Set GOOGLE_API_KEY (environment variable or Colab secret).\")\n",
        "\n",
        "client = genai.Client(api_key=api_key)\n",
        "\n",
        "EMBED_MODEL = \"gemini-embedding-001\"\n",
        "GEN_MODEL = \"gemini-3-flash-preview\"\n",
        "\n",
        "print(\"Gemini client ready\")\n",
        "print(\"Embedding model:\", EMBED_MODEL)\n",
        "print(\"Generation model:\", GEN_MODEL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3Y7wj88MaIDz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y7wj88MaIDz",
        "outputId": "c4483121-a7a2-4c89-8d7f-44631b4623de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded documents: ['gemini_cookbook', 'chroma_readme', 'langchain_readme', 'llamaindex_readme']\n",
            "- gemini_cookbook: 14,088 chars\n",
            "- chroma_readme: 5,475 chars\n",
            "- langchain_readme: 6,867 chars\n",
            "- llamaindex_readme: 13,291 chars\n"
          ]
        }
      ],
      "source": [
        "# A small, thematically coherent corpus for RAG:\n",
        "# public docs/README files about Gemini, vector DBs, and RAG tooling.\n",
        "DOC_URLS = {\n",
        "    \"gemini_cookbook\": \"https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\",\n",
        "    \"chroma_readme\": \"https://raw.githubusercontent.com/chroma-core/chroma/main/README.md\",\n",
        "    \"langchain_readme\": \"https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\",\n",
        "    \"llamaindex_readme\": \"https://raw.githubusercontent.com/run-llama/llama_index/main/README.md\"\n",
        "}\n",
        "\n",
        "def fetch_text(url: str) -> str:\n",
        "    r = requests.get(url, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "\n",
        "raw_docs = {}\n",
        "for name, url in DOC_URLS.items():\n",
        "    raw_docs[name] = fetch_text(url)\n",
        "\n",
        "print(\"Downloaded documents:\", list(raw_docs.keys()))\n",
        "for k, v in raw_docs.items():\n",
        "    print(f\"- {k}: {len(v):,} chars\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "tDFy5OoGaIDz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "tDFy5OoGaIDz",
        "outputId": "bf00139c-4f90-45f7-a803-b5fbd5bc32c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 40\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        id           doc_id  chunk_index  \\\n",
              "0  gemini_cookbook_chunk_0  gemini_cookbook            0   \n",
              "1  gemini_cookbook_chunk_1  gemini_cookbook            1   \n",
              "2  gemini_cookbook_chunk_2  gemini_cookbook            2   \n",
              "\n",
              "                                                text  \\\n",
              "0  # Welcome to the Gemini API Cookbook\\n\\nThis c...   \n",
              "1  ation).\\n> \\n> **üçå Nano-Banana Pro**: Go banan...   \n",
              "2   Practical use cases demonstrating how to comb...   \n",
              "\n",
              "                                          source_url  \n",
              "0  https://raw.githubusercontent.com/google-gemin...  \n",
              "1  https://raw.githubusercontent.com/google-gemin...  \n",
              "2  https://raw.githubusercontent.com/google-gemin...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9821337-9d35-45bd-a3c9-299b77ee9359\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>doc_id</th>\n",
              "      <th>chunk_index</th>\n",
              "      <th>text</th>\n",
              "      <th>source_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gemini_cookbook_chunk_0</td>\n",
              "      <td>gemini_cookbook</td>\n",
              "      <td>0</td>\n",
              "      <td># Welcome to the Gemini API Cookbook\\n\\nThis c...</td>\n",
              "      <td>https://raw.githubusercontent.com/google-gemin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gemini_cookbook_chunk_1</td>\n",
              "      <td>gemini_cookbook</td>\n",
              "      <td>1</td>\n",
              "      <td>ation).\\n&gt; \\n&gt; **üçå Nano-Banana Pro**: Go banan...</td>\n",
              "      <td>https://raw.githubusercontent.com/google-gemin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gemini_cookbook_chunk_2</td>\n",
              "      <td>gemini_cookbook</td>\n",
              "      <td>2</td>\n",
              "      <td>Practical use cases demonstrating how to comb...</td>\n",
              "      <td>https://raw.githubusercontent.com/google-gemin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9821337-9d35-45bd-a3c9-299b77ee9359')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f9821337-9d35-45bd-a3c9-299b77ee9359 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f9821337-9d35-45bd-a3c9-299b77ee9359');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_chunks",
              "summary": "{\n  \"name\": \"df_chunks\",\n  \"rows\": 40,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"chroma_readme_chunk_5\",\n          \"chroma_readme_chunk_2\",\n          \"chroma_readme_chunk_1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"chroma_readme\",\n          \"llamaindex_readme\",\n          \"gemini_cookbook\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 13,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          9,\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"com/docs/overview/oss#contributing)\\n\\n**Release Cadence**\\nWe currently release new tagged versions of the `pypi` and `npm` packages on Mondays. Hotfixes go out at any time during the week.\\n\\n## License\\n\\n[Apache 2.0](./LICENSE)\",\n          \"n these!\\n    ids=[\\\"doc1\\\", \\\"doc2\\\"], # unique for each doc\\n)\\n\\n# Query/search 2 most similar results. You can also .get by id\\nresults = collection.query(\\n    query_texts=[\\\"This is a query document\\\"],\\n    n_results=2,\\n    # where={\\\"metadata_field\\\": \\\"is_equal_to_this\\\"}, # optional filter\\n    # where_document={\\\"$contains\\\":\\\"search_string\\\"}  # optional filter\\n)\\n```\\n\\nLearn about all features on our [Docs](https://docs.trychroma.com)\\n\\n## Features\\n- __Simple__: Fully-typed, fully-tested, fully-documented == happiness\\n- __Integrations__: [`\\ud83e\\udd9c\\ufe0f\\ud83d\\udd17 LangChain`](https://blog.langchain.dev/langchain-chroma/) (python and js), [`\\ud83e\\udd99 LlamaIndex`](https://twitter.com/atroyn/status/1628557389762007040) and more soon\\n- __Dev, Test, Prod__: the same API that runs in your python notebook, scales to your cluster\\n- __Feature-rich__: Queries, filtering, regex and more\\n- __Free & Open Source__: Apache 2.0 Licensed\\n\\n## Use case: ChatGPT for ______\\n\\nFor example, the `\\\"Chat your data\\\"` use case:\\n1. Add documents to your database. You can pass in your own embeddings, embedding function, or let Chroma embed them for you.\\n2. Query relevant documents with natural language.\\n3. Compose documents into the context window of a\",\n          \", powers serverless vector, hybrid, and full-text search. It's extremely fast, cost-effective, scalable and painless. Create a DB and try it out in under 30 seconds with $5 of free credits.\\n\\n[Get started with Chroma Cloud](https://trychroma.com/signup)\\n\\n## API\\n\\nThe core API is only 4 functions (run our [\\ud83d\\udca1 Google Colab](https://colab.research.google.com/drive/1QEzFyqnoFxq7LUGyP1vzR4iLt9PpCDXv?usp=sharing)):\\n\\n```python\\nimport chromadb\\n# setup Chroma in-memory, for easy prototyping. Can add persistence easily!\\nclient = chromadb.Client()\\n\\n# Create collection. get_collection, get_or_create_collection, delete_collection also available!\\ncollection = client.create_collection(\\\"all-my-documents\\\")\\n\\n# Add docs to the collection. Can also update and delete. Row-based API coming soon!\\ncollection.add(\\n    documents=[\\\"This is document1\\\", \\\"This is document2\\\"], # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\\n    metadatas=[{\\\"source\\\": \\\"notion\\\"}, {\\\"source\\\": \\\"google-docs\\\"}], # filter on these!\\n    ids=[\\\"doc1\\\", \\\"doc2\\\"], # unique for each doc\\n)\\n\\n# Query/search 2 most similar results. You can also .get by id\\nresults = collection.query(\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"https://raw.githubusercontent.com/chroma-core/chroma/main/README.md\",\n          \"https://raw.githubusercontent.com/run-llama/llama_index/main/README.md\",\n          \"https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    text = text.replace(\"\\r\", \"\")\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 1200, overlap: int = 150):\n",
        "    text = clean_text(text)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunks.append(text[start:end])\n",
        "        if end == len(text):\n",
        "            break\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "records = []\n",
        "for doc_id, text in raw_docs.items():\n",
        "    chunks = chunk_text(text)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        records.append({\n",
        "            \"id\": f\"{doc_id}_chunk_{i}\",\n",
        "            \"doc_id\": doc_id,\n",
        "            \"chunk_index\": i,\n",
        "            \"text\": chunk,\n",
        "            \"source_url\": DOC_URLS[doc_id],\n",
        "        })\n",
        "\n",
        "df_chunks = pd.DataFrame(records)\n",
        "print(\"Total chunks:\", len(df_chunks))\n",
        "df_chunks.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "LayLxnRvaID0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LayLxnRvaID0",
        "outputId": "aedcaa24-5f51-406d-959d-1e826f78eb67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding count: 40\n",
            "Embedding dimension: 3072\n"
          ]
        }
      ],
      "source": [
        "def extract_embeddings(resp):\n",
        "    # Handles both single and batch embedding response shapes.\n",
        "    if hasattr(resp, \"embeddings\") and resp.embeddings is not None:\n",
        "        out = []\n",
        "        for emb in resp.embeddings:\n",
        "            if hasattr(emb, \"values\"):\n",
        "                out.append(list(emb.values))\n",
        "            elif isinstance(emb, dict) and \"values\" in emb:\n",
        "                out.append(list(emb[\"values\"]))\n",
        "        return out\n",
        "\n",
        "    if hasattr(resp, \"embedding\") and resp.embedding is not None:\n",
        "        emb = resp.embedding\n",
        "        if hasattr(emb, \"values\"):\n",
        "            return [list(emb.values)]\n",
        "        if isinstance(emb, dict) and \"values\" in emb:\n",
        "            return [list(emb[\"values\"])]\n",
        "\n",
        "    raise ValueError(\"Unexpected embedding response format\")\n",
        "\n",
        "texts = df_chunks[\"text\"].tolist()\n",
        "\n",
        "try:\n",
        "    embed_resp = client.models.embed_content(\n",
        "        model=EMBED_MODEL,\n",
        "        contents=texts,\n",
        "        config={\"task_type\": \"RETRIEVAL_DOCUMENT\"},\n",
        "    )\n",
        "except Exception:\n",
        "    embed_resp = client.models.embed_content(\n",
        "        model=EMBED_MODEL,\n",
        "        contents=texts,\n",
        "    )\n",
        "\n",
        "chunk_embeddings = extract_embeddings(embed_resp)\n",
        "\n",
        "print(\"Embedding count:\", len(chunk_embeddings))\n",
        "print(\"Embedding dimension:\", len(chunk_embeddings[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ElxeM5SaaID0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElxeM5SaaID0",
        "outputId": "408f0e4a-5d43-47ae-fd7e-b0305c9cc60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored chunks in Chroma: 40\n"
          ]
        }
      ],
      "source": [
        "# In-memory vector database (temporary).\n",
        "chroma_client = chromadb.Client()\n",
        "collection = chroma_client.get_or_create_collection(name=\"gemini_rag_demo\")\n",
        "\n",
        "collection.add(\n",
        "    ids=df_chunks[\"id\"].tolist(),\n",
        "    documents=df_chunks[\"text\"].tolist(),\n",
        "    embeddings=chunk_embeddings,\n",
        "    metadatas=[\n",
        "        {\n",
        "            \"doc_id\": row.doc_id,\n",
        "            \"chunk_index\": int(row.chunk_index),\n",
        "            \"source_url\": row.source_url,\n",
        "        }\n",
        "        for row in df_chunks.itertuples(index=False)\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Stored chunks in Chroma:\", collection.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "MSm8yj-uaID0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSm8yj-uaID0",
        "outputId": "6079a94f-0016-4e5d-8b8a-112637cee661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "\n",
            "The **Gemini cookbook** is a collection of educational guides and practical examples for building with the Gemini API, covering features like grounding, batch processing, and 3D spatial understanding [1], [2], [7]. In contrast, **Chroma** is a vector database used to store and query embeddings, allowing users to find relevant documents through natural language and compose them into an LLM's context window [3], [4].\n",
            "\n",
            "**LangChain** fits as an orchestration framework and integration layer that:\n",
            "*   Connects LLMs to diverse data sources and vector stores like Chroma for real-time data augmentation [4], [5].\n",
            "*   Provides a modular architecture for model interoperability, rapid prototyping, and agent orchestration (via LangGraph) [5], [6].\n",
            "*   Is used within Gemini-specific applications to power backend agents [8].\n",
            "\n",
            "Citation map:\n",
            "[1] https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\n",
            "[2] https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\n",
            "[3] https://raw.githubusercontent.com/chroma-core/chroma/main/README.md\n",
            "[4] https://raw.githubusercontent.com/chroma-core/chroma/main/README.md\n",
            "[5] https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\n",
            "[6] https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\n",
            "[7] https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\n",
            "[8] https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\n"
          ]
        }
      ],
      "source": [
        "def retrieve(query: str, k: int = 8, n_candidates: int = 24, max_per_doc: int = 2):\n",
        "    try:\n",
        "        q_resp = client.models.embed_content(\n",
        "            model=EMBED_MODEL,\n",
        "            contents=[query],\n",
        "            config={\"task_type\": \"RETRIEVAL_QUERY\"},\n",
        "        )\n",
        "    except Exception:\n",
        "        q_resp = client.models.embed_content(\n",
        "            model=EMBED_MODEL,\n",
        "            contents=[query],\n",
        "        )\n",
        "\n",
        "    q_emb = extract_embeddings(q_resp)[0]\n",
        "\n",
        "    res = collection.query(\n",
        "        query_embeddings=[q_emb],\n",
        "        n_results=n_candidates,\n",
        "    )\n",
        "\n",
        "    ranked = []\n",
        "    for i in range(len(res[\"ids\"][0])):\n",
        "        ranked.append({\n",
        "            \"id\": res[\"ids\"][0][i],\n",
        "            \"document\": res[\"documents\"][0][i],\n",
        "            \"metadata\": res[\"metadatas\"][0][i],\n",
        "            \"distance\": res[\"distances\"][0][i],\n",
        "        })\n",
        "\n",
        "    # Keep top relevance while preventing one source from consuming all slots.\n",
        "    selected = []\n",
        "    per_doc = {}\n",
        "    for r in ranked:\n",
        "        doc_id = r[\"metadata\"][\"doc_id\"]\n",
        "        count = per_doc.get(doc_id, 0)\n",
        "        if count >= max_per_doc:\n",
        "            continue\n",
        "        selected.append(r)\n",
        "        per_doc[doc_id] = count + 1\n",
        "        if len(selected) >= k:\n",
        "            break\n",
        "\n",
        "    # Fallback: if diversity cap is too strict, top up with best remaining chunks.\n",
        "    if len(selected) < k:\n",
        "        selected_ids = {x[\"id\"] for x in selected}\n",
        "        for r in ranked:\n",
        "            if r[\"id\"] in selected_ids:\n",
        "                continue\n",
        "            selected.append(r)\n",
        "            if len(selected) >= k:\n",
        "                break\n",
        "\n",
        "    return selected\n",
        "\n",
        "\n",
        "def answer_with_rag(question: str, k: int = 8):\n",
        "    hits = retrieve(question, k=k)\n",
        "\n",
        "    context_blocks = []\n",
        "    citations = []\n",
        "    for idx, h in enumerate(hits, start=1):\n",
        "        md = h[\"metadata\"]\n",
        "        context_blocks.append(\n",
        "            f\"[Context {idx}] source={md['doc_id']} chunk={md['chunk_index']}\\n{h['document']}\"\n",
        "        )\n",
        "        citations.append((idx, md[\"source_url\"]))\n",
        "\n",
        "    context = \"\\n\\n\".join(context_blocks)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are answering using only the provided context.\n",
        "If the answer is not supported by context, say: \"I don't know based on the provided documents.\"\n",
        "Keep the answer concise and include citation markers like [1], [2] tied to context blocks.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\".strip()\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=GEN_MODEL,\n",
        "        contents=prompt,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"answer\": resp.text,\n",
        "        \"hits\": hits,\n",
        "        \"citations\": citations,\n",
        "    }\n",
        "\n",
        "# Try a question.\n",
        "result = answer_with_rag(\"How do Gemini cookbook and Chroma differ in purpose, and where does LangChain fit?\")\n",
        "print(\"Answer:\\n\")\n",
        "print(result[\"answer\"])\n",
        "\n",
        "print(\"\\nCitation map:\")\n",
        "for idx, url in result[\"citations\"]:\n",
        "    print(f\"[{idx}] {url}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}