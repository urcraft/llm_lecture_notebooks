{
    "cells":  [
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "\u003ca target=\"_blank\" href=\"https://github.com/urcraft/llm_lecture_notebooks/blob/main/03_Gemini_API_Basics_Key_Setup_and_First_Prompts.ipynb\"\u003e",
                                     "  \u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e",
                                     "\u003c/a\u003e"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "SSdMAJbEN8dX",
                      "metadata":  {
                                       "id":  "SSdMAJbEN8dX"
                                   },
                      "source":  [
                                     "# Gemini API Basics: Free Key, First Calls, and Prompting\n",
                                     "\n",
                                     "## What you will learn\n",
                                     "- What a closed-model API is and why teams use it.\n",
                                     "- How to use a free Gemini API key in Colab.\n",
                                     "- How prompt changes affect model outputs.\n",
                                     "\n",
                                     "Expected runtime: 30-45 minutes\n",
                                     "Expected cost: Free tier if usage stays within quota.\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "qvdmzvtdN8dZ",
                      "metadata":  {
                                       "id":  "qvdmzvtdN8dZ"
                                   },
                      "source":  [
                                     "## Setup notes\n",
                                     "1. Store your key as a Colab Secret named GOOGLE_API_KEY.\n",
                                     "2. If key setup fails, you can still complete analysis tasks using reference outputs.\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "TTKeMeCQN8dc",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "TTKeMeCQN8dc",
                                       "outputId":  "47496360-011c-4325-fa89-1a824c3dbfc6"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "%pip -q install -U google-genai\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "hTrJhCHiN8dd",
                      "metadata":  {
                                       "id":  "hTrJhCHiN8dd"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import os\n",
                                     "import time\n",
                                     "import pandas as pd\n",
                                     "\n",
                                     "GEMINI_AVAILABLE = False\n",
                                     "GEMINI_ERROR = None\n",
                                     "\n",
                                     "try:\n",
                                     "    from google import genai\n",
                                     "\n",
                                     "    api_key = os.getenv(\u0027GOOGLE_API_KEY\u0027)\n",
                                     "    if not api_key:\n",
                                     "        try:\n",
                                     "            from google.colab import userdata\n",
                                     "            api_key = userdata.get(\u0027GOOGLE_API_KEY\u0027)\n",
                                     "        except Exception:\n",
                                     "            api_key = None\n",
                                     "\n",
                                     "    if not api_key:\n",
                                     "        raise ValueError(\u0027GOOGLE_API_KEY not found. Set env var or Colab secret GOOGLE_API_KEY.\u0027)\n",
                                     "\n",
                                     "    client = genai.Client(api_key=api_key)\n",
                                     "    GEMINI_AVAILABLE = True\n",
                                     "except Exception as e:\n",
                                     "    GEMINI_ERROR = str(e)\n",
                                     "    print(\u0027Gemini is not available in this runtime.\u0027)\n",
                                     "    print(\u0027Reason:\u0027, GEMINI_ERROR)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "-kDyJwMmN8de",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "-kDyJwMmN8de",
                                       "outputId":  "279a0418-6777-4c0d-c52a-e9470e9fca5b"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "MODEL_ID = \u0027gemini-2.5-flash\u0027\n",
                                     "print(\u0027Model:\u0027, MODEL_ID)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "JR4hhwv2N8df",
                      "metadata":  {
                                       "id":  "JR4hhwv2N8df"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "def run_gemini(prompt: str, system_instruction=None):\n",
                                     "    if not GEMINI_AVAILABLE:\n",
                                     "        return {\n",
                                     "            \u0027ok\u0027: False,\n",
                                     "            \u0027model\u0027: MODEL_ID,\n",
                                     "            \u0027prompt\u0027: prompt,\n",
                                     "            \u0027system_instruction\u0027: system_instruction,\n",
                                     "            \u0027text\u0027: None,\n",
                                     "            \u0027latency_s\u0027: None,\n",
                                     "            \u0027tokens_in\u0027: None,\n",
                                     "            \u0027error\u0027: GEMINI_ERROR,\n",
                                     "        }\n",
                                     "\n",
                                     "    config = {\u0027system_instruction\u0027: system_instruction} if system_instruction else None\n",
                                     "\n",
                                     "    try:\n",
                                     "        start = time.perf_counter()\n",
                                     "        response = client.models.generate_content(model=MODEL_ID, contents=prompt, config=config)\n",
                                     "        latency = time.perf_counter() - start\n",
                                     "\n",
                                     "        token_info = client.models.count_tokens(model=MODEL_ID, contents=prompt)\n",
                                     "        tokens_in = getattr(token_info, \u0027total_tokens\u0027, None)\n",
                                     "\n",
                                     "        return {\n",
                                     "            \u0027ok\u0027: True,\n",
                                     "            \u0027model\u0027: MODEL_ID,\n",
                                     "            \u0027prompt\u0027: prompt,\n",
                                     "            \u0027system_instruction\u0027: system_instruction,\n",
                                     "            \u0027text\u0027: response.text,\n",
                                     "            \u0027latency_s\u0027: round(latency, 2),\n",
                                     "            \u0027tokens_in\u0027: tokens_in,\n",
                                     "            \u0027error\u0027: None,\n",
                                     "        }\n",
                                     "    except Exception as e:\n",
                                     "        return {\n",
                                     "            \u0027ok\u0027: False,\n",
                                     "            \u0027model\u0027: MODEL_ID,\n",
                                     "            \u0027prompt\u0027: prompt,\n",
                                     "            \u0027system_instruction\u0027: system_instruction,\n",
                                     "            \u0027text\u0027: None,\n",
                                     "            \u0027latency_s\u0027: None,\n",
                                     "            \u0027tokens_in\u0027: None,\n",
                                     "            \u0027error\u0027: str(e),\n",
                                     "        }\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "1a219189",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "1a219189",
                                       "outputId":  "293332e7-cea4-484e-d5a6-1df9d3236bd2"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# First basic call (no system prompt)\n",
                                     "first_prompt = \u0027Explain GPU and VRAM to a high-school student in 5 bullet points.\u0027\n",
                                     "first_result = run_gemini(first_prompt)\n",
                                     "\n",
                                     "print(\u0027--- First Gemini Call ---\u0027)\n",
                                     "print(\u0027Prompt:\u0027, first_prompt)\n",
                                     "print(\u0027Error:\u0027, first_result[\u0027error\u0027])\n",
                                     "print(\u0027Output:\\n\u0027)\n",
                                     "print(first_result[\u0027text\u0027])\n",
                                     "\n",
                                     "# Prompt for system prompt comparison\n",
                                     "comparison_prompt = \u0027Explain the concept of large language models (LLMs).\u0027\n",
                                     "system_prompt_1 = \u0027You are a helpful assistant that provides concise answers.\u0027\n",
                                     "system_prompt_2 = \u0027You are a helpful assistant that provides detailed explanations, especially for technical terms.\u0027\n",
                                     "\n",
                                     "print(\u0027\\n--- System Prompt Comparison Setup ---\u0027)\n",
                                     "print(f\u0027User prompt: {comparison_prompt}\u0027)\n",
                                     "print(f\u0027System Prompt 1: {system_prompt_1}\u0027)\n",
                                     "print(f\u0027System Prompt 2: {system_prompt_2}\u0027)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "18f120de",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/",
                                                     "height":  1000
                                                 },
                                       "id":  "18f120de",
                                       "outputId":  "2cedeccc-e64b-4e62-bba0-a2ca186a0770"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "baseline_result = run_gemini(comparison_prompt)\n",
                                     "result_sp1 = run_gemini(comparison_prompt, system_instruction=system_prompt_1)\n",
                                     "result_sp2 = run_gemini(comparison_prompt, system_instruction=system_prompt_2)\n",
                                     "\n",
                                     "print(\u0027\\n--- Output: No System Prompt ---\u0027)\n",
                                     "print(baseline_result[\u0027text\u0027])\n",
                                     "\n",
                                     "print(\u0027\\n--- Output: System Prompt 1 (Concise) ---\u0027)\n",
                                     "print(result_sp1[\u0027text\u0027])\n",
                                     "\n",
                                     "print(\u0027\\n--- Output: System Prompt 2 (Detailed) ---\u0027)\n",
                                     "print(result_sp2[\u0027text\u0027])\n",
                                     "\n",
                                     "comparison_df = pd.DataFrame([\n",
                                     "    {\u0027prompt_type\u0027: \u0027No System Prompt\u0027, \u0027latency_s\u0027: baseline_result[\u0027latency_s\u0027], \u0027tokens_in\u0027: baseline_result[\u0027tokens_in\u0027], \u0027error\u0027: baseline_result[\u0027error\u0027], \u0027text\u0027: baseline_result[\u0027text\u0027]},\n",
                                     "    {\u0027prompt_type\u0027: \u0027System Prompt 1: Concise\u0027, \u0027latency_s\u0027: result_sp1[\u0027latency_s\u0027], \u0027tokens_in\u0027: result_sp1[\u0027tokens_in\u0027], \u0027error\u0027: result_sp1[\u0027error\u0027], \u0027text\u0027: result_sp1[\u0027text\u0027]},\n",
                                     "    {\u0027prompt_type\u0027: \u0027System Prompt 2: Detailed\u0027, \u0027latency_s\u0027: result_sp2[\u0027latency_s\u0027], \u0027tokens_in\u0027: result_sp2[\u0027tokens_in\u0027], \u0027error\u0027: result_sp2[\u0027error\u0027], \u0027text\u0027: result_sp2[\u0027text\u0027]}\n",
                                     "])\n",
                                     "\n",
                                     "comparison_df\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "jUtCZuERN8df",
                      "metadata":  {
                                       "id":  "jUtCZuERN8df"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# Additional prompt examples for basic API calling\n",
                                     "prompts = [\n",
                                     "    \u0027What is the difference between open-weight and closed models? Keep it simple.\u0027,\n",
                                     "    \u0027Compare open-weight and closed models for a small startup. Include privacy, cost, and control in a table.\u0027\n",
                                     "]\n",
                                     "\n",
                                     "rows = [run_gemini(p) for p in prompts]\n",
                                     "results_df = pd.DataFrame(rows)\n",
                                     "results_df[[\u0027prompt\u0027, \u0027latency_s\u0027, \u0027tokens_in\u0027, \u0027error\u0027, \u0027text\u0027]]\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "5UKBi0jpN8df",
                      "metadata":  {
                                       "id":  "5UKBi0jpN8df"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# Optional: quick side-by-side excerpt view for teaching\n",
                                     "preview_df = comparison_df.copy()\n",
                                     "preview_df[\u0027text_preview\u0027] = preview_df[\u0027text\u0027].fillna(\u0027\u0027).str.slice(0, 240)\n",
                                     "preview_df[[\u0027prompt_type\u0027, \u0027latency_s\u0027, \u0027tokens_in\u0027, \u0027error\u0027, \u0027text_preview\u0027]]\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "NpjJOIQAN8dg",
                      "metadata":  {
                                       "id":  "NpjJOIQAN8dg"
                                   },
                      "source":  [
                                     "## Checkpoint\n",
                                     "- Rewrite a weak prompt into a strong prompt for the same task.\n",
                                     "- Note 2 output differences you observe.\n",
                                     "\n",
                                     "## Reflection\n",
                                     "- When would a closed API model be better than running local models?\n",
                                     "\n",
                                     "## Troubleshooting\n",
                                     "- If key lookup fails, re-open Colab Secrets and verify GOOGLE_API_KEY.\n",
                                     "- Free-tier quota limits can cause temporary request failures.\n"
                                 ]
                  }
              ],
    "metadata":  {
                     "colab":  {
                                   "gpuType":  "T4",
                                   "provenance":  [

                                                  ]
                               },
                     "kernelspec":  {
                                        "display_name":  "Python 3 (ipykernel)",
                                        "language":  "python",
                                        "name":  "python3"
                                    },
                     "language_info":  {
                                           "codemirror_mode":  {
                                                                   "name":  "ipython",
                                                                   "version":  3
                                                               },
                                           "file_extension":  ".py",
                                           "mimetype":  "text/x-python",
                                           "name":  "python",
                                           "nbconvert_exporter":  "python",
                                           "pygments_lexer":  "ipython3",
                                           "version":  "3.12.12"
                                       }
                 },
    "nbformat":  4,
    "nbformat_minor":  5
}
