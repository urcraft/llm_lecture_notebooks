{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxgdTxgPeZdy"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/urcraft/llm_lecture_notebooks/blob/main/07_Gemini_Langfuse_Tracing_Basics.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ],
      "id": "vxgdTxgPeZdy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zhywkTmeZd2"
      },
      "source": [
        "# Gemini + Langfuse: Logging and Tracing Gemini Calls\n",
        "\n",
        "This notebook shows how to log and trace Gemini API calls in Langfuse using automatic OpenTelemetry instrumentation.\n"
      ],
      "id": "2zhywkTmeZd2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxh_VMnZeZd3"
      },
      "source": [
        "## What you will learn\n",
        "\n",
        "- How to configure Gemini and Langfuse credentials via Google Colab Secrets.\n",
        "- How to enable automatic tracing for the Google GenAI SDK.\n",
        "- How to run Gemini calls and verify traces in the Langfuse UI.\n"
      ],
      "id": "jxh_VMnZeZd3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fnbu1oteZd3"
      },
      "source": [
        "## Setup notes\n",
        "\n",
        "1. Add these secrets in Colab (`Tools -> Secrets`):\n",
        "   - `GOOGLE_API_KEY`\n",
        "   - `LANGFUSE_PUBLIC_KEY`\n",
        "   - `LANGFUSE_SECRET_KEY`\n",
        "2. Optional secret:\n",
        "   - `LANGFUSE_BASE_URL` (EU default: `https://cloud.langfuse.com`, US: `https://us.cloud.langfuse.com`)\n",
        "3. You can also set these as environment variables.\n"
      ],
      "id": "7fnbu1oteZd3"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAoMtK41eZd4",
        "outputId": "d6c5a53a-ca95-41f8-d0b7-6b0761af50d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.8/728.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.4/420.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.1 which is incompatible.\n",
            "gradio 5.50.0 requires pandas<3.0,>=1.0, but you have pandas 3.0.1 which is incompatible.\n",
            "db-dtypes 1.5.0 requires pandas<3.0.0,>=1.5.3, but you have pandas 3.0.1 which is incompatible.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip -q install -U google-genai langfuse openinference-instrumentation-google-genai pandas"
      ],
      "id": "zAoMtK41eZd4"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uF7inqIZeZd5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "SDK_AVAILABLE = False\n",
        "SDK_ERROR = None\n",
        "\n",
        "try:\n",
        "    from langfuse import get_client\n",
        "    from openinference.instrumentation.google_genai import GoogleGenAIInstrumentor\n",
        "    from google import genai\n",
        "    SDK_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    SDK_ERROR = e\n",
        "    print('Import/setup warning:', repr(e))\n"
      ],
      "id": "uF7inqIZeZd5"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Up3QBHeZd6",
        "outputId": "1140e80d-2537-4d1e-db39-2570b55c394f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secrets loaded successfully.\n",
            "LANGFUSE_BASE_URL = https://cloud.langfuse.com\n"
          ]
        }
      ],
      "source": [
        "def get_secret(name: str, default=None):\n",
        "    value = os.getenv(name)\n",
        "    if value:\n",
        "        return value\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        value = userdata.get(name)\n",
        "        if value:\n",
        "            return value\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return default\n",
        "\n",
        "GOOGLE_API_KEY = get_secret('GOOGLE_API_KEY')\n",
        "LANGFUSE_PUBLIC_KEY = get_secret('LANGFUSE_PUBLIC_KEY')\n",
        "LANGFUSE_SECRET_KEY = get_secret('LANGFUSE_SECRET_KEY')\n",
        "LANGFUSE_BASE_URL = get_secret('LANGFUSE_BASE_URL', 'https://cloud.langfuse.com')\n",
        "\n",
        "missing = [\n",
        "    name for name, value in [\n",
        "        ('GOOGLE_API_KEY', GOOGLE_API_KEY),\n",
        "        ('LANGFUSE_PUBLIC_KEY', LANGFUSE_PUBLIC_KEY),\n",
        "        ('LANGFUSE_SECRET_KEY', LANGFUSE_SECRET_KEY),\n",
        "    ]\n",
        "    if not value\n",
        "]\n",
        "\n",
        "if missing:\n",
        "    raise ValueError(\n",
        "        'Missing required secrets/env vars: ' + ', '.join(missing) +\n",
        "        '. Add them in Colab Secrets or set environment variables.'\n",
        "    )\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "os.environ['LANGFUSE_PUBLIC_KEY'] = LANGFUSE_PUBLIC_KEY\n",
        "os.environ['LANGFUSE_SECRET_KEY'] = LANGFUSE_SECRET_KEY\n",
        "os.environ['LANGFUSE_BASE_URL'] = LANGFUSE_BASE_URL\n",
        "\n",
        "print('Secrets loaded successfully.')\n",
        "print('LANGFUSE_BASE_URL =', LANGFUSE_BASE_URL)\n"
      ],
      "id": "L3Up3QBHeZd6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxTw0uqeeZd6",
        "outputId": "5faaa6f0-3323-40c6-8e33-9055d841c4e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Langfuse authentication OK.\n",
            "Gemini instrumentation enabled.\n"
          ]
        }
      ],
      "source": [
        "if not SDK_AVAILABLE:\n",
        "    raise RuntimeError(f'SDK imports failed: {SDK_ERROR!r}')\n",
        "\n",
        "# Initialize Langfuse client and check credentials.\n",
        "langfuse = get_client()\n",
        "assert langfuse.auth_check(), 'Langfuse auth failed. Verify keys and LANGFUSE_BASE_URL.'\n",
        "\n",
        "# Enable automatic tracing for Google GenAI SDK calls.\n",
        "GoogleGenAIInstrumentor().instrument()\n",
        "\n",
        "print('Langfuse authentication OK.')\n",
        "print('Gemini instrumentation enabled.')\n"
      ],
      "id": "SxTw0uqeeZd6"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWhVtY4JeZd7",
        "outputId": "50463817-5daf-4f61-c4ee-fc75f9c27b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: gemini-2.5-flash\n"
          ]
        }
      ],
      "source": [
        "MODEL_ID = 'gemini-2.5-flash'\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "print('Model:', MODEL_ID)\n"
      ],
      "id": "oWhVtY4JeZd7"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf0USGYZeZd7",
        "outputId": "d6dd9875-12ed-4eab-f064-3b58f377aadf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success: True\n",
            "Latency (s): 8.582\n",
            "Response: Observability for LLM applications extends traditional software observability to specifically address the unique challenges and characteristics of generative AI:\n",
            "\n",
            "*   **Logs & Context:** Capturing comprehensive records of all inputs (user prompts, system prompts, retrieved context), intermediate steps (RAG retrievals, tool calls, moderation results), and LLM outputs for every request. This allows for detailed post-hoc analysis, debugging of unexpected behavior, and understanding the LLM's reasoning path.\n",
            "*   **Performance Metrics:** Monitoring key quantitative indicators such as latency per LLM call and overall request, token consumption (input, output, total) for cost tracking, API error rates, and throughput. These metrics are crucial for identifying performance bottlenecks, managing operational costs, and optimizing resource usage.\n",
            "*   **End-to-End Tracing:** Visualizing the complete flow of a user request through multi-step LLM chains, custom application logic, and external integrations (e.g., databases, external APIs). This helps pinpoint bottlenecks, identify dependencies, and understand the cascade effect of issues in complex, multi-component LLM applications.\n",
            "*   **Output Quality & Behavior Monitoring:** Evaluating the semantic quality and safety of LLM responses, including automated or human detection of hallucinations, factual inaccuracies, bias, prompt injection attempts, safety violations, and adherence to desired persona or format. This is vital for ensuring the LLM application consistently provides accurate, safe, and helpful information.\n"
          ]
        }
      ],
      "source": [
        "def run_gemini(prompt: str):\n",
        "    start = time.perf_counter()\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=prompt,\n",
        "        )\n",
        "        elapsed = time.perf_counter() - start\n",
        "        return {\n",
        "            'ok': True,\n",
        "            'prompt': prompt,\n",
        "            'text': (response.text or '').strip(),\n",
        "            'latency_s': round(elapsed, 3),\n",
        "        }\n",
        "    except Exception as e:\n",
        "        elapsed = time.perf_counter() - start\n",
        "        return {\n",
        "            'ok': False,\n",
        "            'prompt': prompt,\n",
        "            'text': f'ERROR: {e}',\n",
        "            'latency_s': round(elapsed, 3),\n",
        "        }\n",
        "\n",
        "first_result = run_gemini('In 4 bullet points, explain what observability means for LLM applications.')\n",
        "print('Success:', first_result['ok'])\n",
        "print('Latency (s):', first_result['latency_s'])\n",
        "print('Response:', first_result['text'])\n"
      ],
      "id": "sf0USGYZeZd7"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "6C5RBxaIeZd7",
        "outputId": "0685ce74-6de8-47a6-c57a-c4c3fd9cc6f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     ok  latency_s                                             prompt  \\\n",
              "0  True      4.811  Give a one-sentence definition of tracing for ...   \n",
              "1  True      5.920  Name three reasons to monitor prompt/response ...   \n",
              "2  True      8.142  Write a short note to students on why logs + t...   \n",
              "\n",
              "                                    response_preview  \n",
              "0  Tracing for LLM apps is the process of recordi...  \n",
              "1  Here are three reasons to monitor prompt/respo...  \n",
              "2  Hey Students,\\n\\nEver wondered how to debug a ...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ok</th>\n",
              "      <th>latency_s</th>\n",
              "      <th>prompt</th>\n",
              "      <th>response_preview</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True</td>\n",
              "      <td>4.811</td>\n",
              "      <td>Give a one-sentence definition of tracing for ...</td>\n",
              "      <td>Tracing for LLM apps is the process of recordi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>True</td>\n",
              "      <td>5.920</td>\n",
              "      <td>Name three reasons to monitor prompt/response ...</td>\n",
              "      <td>Here are three reasons to monitor prompt/respo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True</td>\n",
              "      <td>8.142</td>\n",
              "      <td>Write a short note to students on why logs + t...</td>\n",
              "      <td>Hey Students,\\n\\nEver wondered how to debug a ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "prompts = [\n",
        "    'Give a one-sentence definition of tracing for LLM apps.',\n",
        "    'Name three reasons to monitor prompt/response latency.',\n",
        "    'Write a short note to students on why logs + traces help debugging.'\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for p in prompts:\n",
        "    out = run_gemini(p)\n",
        "    rows.append({\n",
        "        'ok': out['ok'],\n",
        "        'latency_s': out['latency_s'],\n",
        "        'prompt': p,\n",
        "        'response_preview': (out['text'][:160] + '...') if len(out['text']) > 160 else out['text'],\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows)\n",
        "results_df\n"
      ],
      "id": "6C5RBxaIeZd7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "a2694981",
        "outputId": "b9b86f07-6d48-4729-ad1e-26a1ed4686f3"
      },
      "source": [
        "results_df.to_excel('results.xlsx', index=False)\n",
        "print('DataFrame saved to results.xlsx')\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('results.xlsx')\n",
        "except Exception:\n",
        "    print('If not running in Colab, download the file from the notebook working directory.')"
      ],
      "id": "a2694981",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame saved to results.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_912e2082-3cf2-45df-ae74-dc4ae1b0221e\", \"results.xlsx\", 5330)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV6ohI1qeZd8"
      },
      "source": [
        "## View traces in Langfuse\n",
        "\n",
        "After running the calls above:\n",
        "\n",
        "1. Open your Langfuse project.\n",
        "2. Go to the Trace view/table.\n",
        "3. Filter to recent traces and inspect entries from this notebook run.\n",
        "4. Confirm you can see model calls, inputs, outputs, and timing metadata.\n",
        "\n",
        "Tip: If traces do not appear immediately, wait a few seconds and refresh.\n"
      ],
      "id": "uV6ohI1qeZd8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrYtHiiZeZd8"
      },
      "source": [
        "## Checkpoint\n",
        "\n",
        "- Did at least one Gemini call produce output in the notebook?\n",
        "- Do you see matching traces in Langfuse?\n",
        "- Which prompt had the highest latency and why might that happen?\n"
      ],
      "id": "PrYtHiiZeZd8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NOszoIGeZd8"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "- `Missing required secrets/env vars`: verify secret names exactly.\n",
        "- `Langfuse auth failed`: check `LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, and `LANGFUSE_BASE_URL`.\n",
        "- No traces visible: ensure the instrumentation cell ran before Gemini calls.\n",
        "- Colab runtime reset: rerun all setup cells after reset.\n"
      ],
      "id": "6NOszoIGeZd8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}