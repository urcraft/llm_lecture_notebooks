{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "_d4F0mNomWCQ",
   "metadata": {
    "id": "_d4F0mNomWCQ"
   },
   "source": [
    "# Open vs Closed Models: Gemini vs Local Comparison\n",
    "\n",
    "## What you will learn\n",
    "- How to compare model outputs with a simple rubric.\n",
    "- Tradeoffs: quality, latency, cost proxy, and privacy.\n",
    "\n",
    "Expected runtime: 30-45 minutes\n",
    "Expected cost: Gemini free tier plus local runtime resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850693b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "850693b4",
    "outputId": "42745ed1-6ce6-48ef-8b13-e8b3ce2334c0"
   },
   "outputs": [],
   "source": [
    "%pip -q install -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13db40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b13db40",
    "outputId": "a4d2f133-0f47-423e-d9f7-4d2ca7b01540"
   },
   "outputs": [],
   "source": [
    "!sudo apt -qq update\n",
    "!sudo apt -qq install -y pciutils zstd\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "import time\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "LOCAL_MODEL = 'qwen3:4b'\n",
    "MODEL_ID = 'gemini-3-flash-preview'\n",
    "\n",
    "def start_ollama_service():\n",
    "    subprocess.Popen(['ollama', 'serve'])\n",
    "    time.sleep(5)\n",
    "\n",
    "def pull_local_model(model_name: str):\n",
    "    try:\n",
    "        subprocess.run(['ollama', 'pull', model_name], check=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print('Could not pull local model:', e)\n",
    "        return False\n",
    "\n",
    "start_ollama_service()\n",
    "LOCAL_AVAILABLE = pull_local_model(LOCAL_MODEL)\n",
    "print('LOCAL_AVAILABLE =', LOCAL_AVAILABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3196528",
   "metadata": {
    "id": "d3196528"
   },
   "outputs": [],
   "source": [
    "GEMINI_AVAILABLE = False\n",
    "GEMINI_ERROR = None\n",
    "\n",
    "try:\n",
    "    from google import genai\n",
    "    import os\n",
    "    HARDCODED_GOOGLE_API_KEY = None\n",
    "    api_key = HARDCODED_GOOGLE_API_KEY or os.getenv('GOOGLE_API_KEY')\n",
    "    if not api_key:\n",
    "        from google.colab import userdata\n",
    "        api_key = userdata.get('GOOGLE_API_KEY')\n",
    "    if not api_key:\n",
    "        raise ValueError('Set HARDCODED_GOOGLE_API_KEY or GOOGLE_API_KEY')\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    GEMINI_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    GEMINI_ERROR = str(e)\n",
    "    print('Gemini unavailable:', GEMINI_ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e528b4",
   "metadata": {
    "id": "38e528b4"
   },
   "outputs": [],
   "source": [
    "def run_local(prompt: str):\n",
    "    start = time.perf_counter()\n",
    "    if not LOCAL_AVAILABLE:\n",
    "        return {'ok': False, 'model': LOCAL_MODEL, 'output': None, 'latency_s': None, 'error': 'Local model unavailable'}\n",
    "    try:\n",
    "        proc = subprocess.run(['ollama', 'run', LOCAL_MODEL, prompt], capture_output=True, text=True, check=True)\n",
    "        return {'ok': True, 'model': LOCAL_MODEL, 'output': proc.stdout.strip(), 'latency_s': round(time.perf_counter() - start, 2), 'error': None}\n",
    "    except Exception as e:\n",
    "        return {'ok': False, 'model': LOCAL_MODEL, 'output': None, 'latency_s': None, 'error': str(e)}\n",
    "\n",
    "\n",
    "def run_gemini(prompt: str):\n",
    "    start = time.perf_counter()\n",
    "    if not GEMINI_AVAILABLE:\n",
    "        return {'ok': False, 'model': MODEL_ID, 'output': None, 'latency_s': None, 'error': GEMINI_ERROR}\n",
    "    try:\n",
    "        response = client.models.generate_content(model=MODEL_ID, contents=prompt)\n",
    "        return {'ok': True, 'model': MODEL_ID, 'output': response.text, 'latency_s': round(time.perf_counter() - start, 2), 'error': None}\n",
    "    except Exception as e:\n",
    "        return {'ok': False, 'model': MODEL_ID, 'output': None, 'latency_s': None, 'error': str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ff61a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "9b7ff61a",
    "outputId": "c2d3a8fd-5059-42a3-ea1f-804c195fba66"
   },
   "outputs": [],
   "source": [
    "TASKS = [\n",
    "    'A Danish mid-sized B2B company wants to deploy a customer-support AI assistant. Give a one-paragraph recommendation on whether to start with an open-weight model or a closed model, and why.',\n",
    "    'You are advising an enterprise buyer in Denmark. In one paragraph, compare GPU vs CPU choices for running AI inference in production, focusing on cost, latency, and scalability tradeoffs.',\n",
    "    'In one paragraph, propose a simple pilot plan for evaluating two LLM vendors for internal knowledge search in a regulated enterprise setting (success metrics, timeline, and key risks).'\n",
    "]\n",
    "\n",
    "local_rows = []\n",
    "for task in TASKS:\n",
    "    local_res = run_local(task)\n",
    "    local_rows.append({\n",
    "        'task': task,\n",
    "        'model': local_res['model'],\n",
    "        'output': local_res['output'],\n",
    "        'latency_s': local_res['latency_s'],\n",
    "        'error': local_res['error']\n",
    "    })\n",
    "\n",
    "local_df = pd.DataFrame(local_rows)\n",
    "print(f'Local run completed for {len(local_df)} tasks.')\n",
    "local_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860e837",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 795
    },
    "id": "3860e837",
    "outputId": "10c08e11-1175-4b0a-f7a2-31f09ae510ad"
   },
   "outputs": [],
   "source": [
    "if 'TASKS' not in globals() or 'local_df' not in globals():\n",
    "    raise RuntimeError('Run the local-task cell first to create TASKS and local_df.')\n",
    "\n",
    "gemini_rows = []\n",
    "failed_gemini_rows = []\n",
    "\n",
    "for task in TASKS:\n",
    "    gem_res = run_gemini(task)\n",
    "    if gem_res['ok']:\n",
    "        gemini_rows.append({\n",
    "            'task': task,\n",
    "            'model': gem_res['model'],\n",
    "            'output': gem_res['output'],\n",
    "            'latency_s': gem_res['latency_s'],\n",
    "            'error': gem_res['error']\n",
    "        })\n",
    "    else:\n",
    "        failed_gemini_rows.append({\n",
    "            'task': task,\n",
    "            'model': MODEL_ID,\n",
    "            'error': gem_res['error']\n",
    "        })\n",
    "\n",
    "gemini_df = pd.DataFrame(gemini_rows)\n",
    "failed_gemini_df = pd.DataFrame(failed_gemini_rows)\n",
    "\n",
    "if not failed_gemini_df.empty:\n",
    "    print('Some Gemini calls failed. Re-run this Gemini cell to retry failed tasks.')\n",
    "    failed_gemini_df\n",
    "else:\n",
    "    print('Gemini run completed for all tasks.')\n",
    "\n",
    "comparison_df = pd.concat([local_df, gemini_df], ignore_index=True)\n",
    "print(f'Rows included in comparison_df: {len(comparison_df)} (Local: {len(local_df)}, Gemini successful: {len(gemini_df)})')\n",
    "comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df['quality_score_1_to_5'] = ''\n",
    "comparison_df['factuality_score_1_to_5'] = ''\n",
    "comparison_df['notes'] = ''\n",
    "\n",
    "export_path = 'comparison_df_student_ratings.xlsx'\n",
    "comparison_df.to_excel(export_path, index=False)\n",
    "print(f'Saved {export_path}')\n",
    "\n",
    "if 'failed_gemini_df' in globals() and not failed_gemini_df.empty:\n",
    "    print('Note: Failed Gemini rows were excluded from the exported XLSX. Re-run the Gemini cell if needed.')\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(export_path)\n",
    "except Exception:\n",
    "    print('If not running in Colab, download the file from the notebook working directory.')\n",
    "\n",
    "comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w3Xoud20mWCa",
   "metadata": {
    "id": "w3Xoud20mWCa"
   },
   "source": [
    "## Checkpoint\n",
    "- Fill in the scoring columns for each output.\n",
    "- Identify one task where local wins and one where Gemini wins.\n",
    "\n",
    "## Reflection\n",
    "- Which model would you choose for: (a) sensitive data, (b) fastest setup, (c) predictable cost?\n",
    "\n",
    "## Troubleshooting\n",
    "- If local model pull fails, verify runtime and available memory.\n",
    "- If Gemini fails, re-run the Gemini cell; failed Gemini rows are not exported to XLSX.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
