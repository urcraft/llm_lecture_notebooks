{
    "cells":  [
                  {
                      "cell_type":  "markdown",
                      "metadata":  {

                                   },
                      "source":  [
                                     "\u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/urcraft/llm_lecture_notebooks/blob/main/06_Gemini_API_RAG_InMemory_VectorDB.ipynb\"\u003e   \u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e \u003c/a\u003e"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "CIVlEH-2aIDv",
                      "metadata":  {
                                       "id":  "CIVlEH-2aIDv"
                                   },
                      "source":  [
                                     "# Gemini API + In-Memory Vector DB (RAG)\n",
                                     "\n",
                                     "## What you will build\n",
                                     "- A minimal Retrieval-Augmented Generation (RAG) pipeline.\n",
                                     "- Gemini embeddings for chunked documents.\n",
                                     "- Chroma in-memory vector database for retrieval.\n",
                                     "- Gemini answer generation grounded in retrieved context.\n",
                                     "\n",
                                     "Expected runtime: 10-20 minutes\n",
                                     "Expected cost: Zero for Gemini Free tier API usage\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "pc5eANXNaIDx",
                      "metadata":  {
                                       "id":  "pc5eANXNaIDx"
                                   },
                      "source":  [
                                     "## Online References Used\n",
                                     "- Gemini embeddings API reference: https://ai.google.dev/api/embeddings\n",
                                     "- Gemini text generation guide: https://ai.google.dev/gemini-api/docs/system-instructions\n",
                                     "- Chroma in-memory client docs: https://docs.trychroma.com/docs/run-chroma/clients\n",
                                     "- Chroma Python reference: https://docs.trychroma.com/reference/python\n",
                                     "- Gemini cookbook repository: https://github.com/google-gemini/cookbook\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  1,
                      "id":  "pVRI_403aIDx",
                      "metadata":  {
                                       "id":  "pVRI_403aIDx"
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m728.8/728.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                                                       "\u001b[?25h\u001b[31mERROR: pip\u0027s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
                                                       "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api\u003e=1.35.0, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
                                                       "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk\u003c1.39.0,\u003e=1.35.0, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
                                                       "grain 0.2.15 requires protobuf\u003e=5.28.3, but you have protobuf 4.25.8 which is incompatible.\n",
                                                       "google-adk 1.25.0 requires opentelemetry-api\u003c1.40.0,\u003e=1.36.0, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
                                                       "google-adk 1.25.0 requires opentelemetry-exporter-otlp-proto-http\u003e=1.36.0, but you have opentelemetry-exporter-otlp-proto-http 1.22.0 which is incompatible.\n",
                                                       "google-adk 1.25.0 requires opentelemetry-sdk\u003c1.40.0,\u003e=1.36.0, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
                                                       "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
                                                       "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
                                                       "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
                                                       "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
                                                       "google-cloud-pubsub 2.35.0 requires opentelemetry-api\u003e=1.27.0, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
                                                       "google-cloud-pubsub 2.35.0 requires opentelemetry-sdk\u003e=1.27.0, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
                                                       "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.22.0 which is incompatible.\n",
                                                       "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
                                                       "grpcio-status 1.71.2 requires protobuf\u003c6.0dev,\u003e=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
                                                       "ydf 0.15.0 requires protobuf\u003c7.0.0,\u003e=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
                                                       "\u001b[0m"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Force-install compatible versions for the Colab environment\n",
                                     "%pip install -q -U google-genai chromadb \\\n",
                                     "    \"pandas==2.2.2\" \\\n",
                                     "    \"requests==2.32.4\" \\\n",
                                     "    \"opentelemetry-sdk==1.22.0\" \\\n",
                                     "    \"opentelemetry-exporter-otlp-proto-http==1.22.0\"\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  2,
                      "id":  "GrWsPjfKaIDy",
                      "metadata":  {
                                       "id":  "GrWsPjfKaIDy"
                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import os\n",
                                     "import re\n",
                                     "import textwrap\n",
                                     "import requests\n",
                                     "import pandas as pd\n",
                                     "import chromadb\n",
                                     "\n",
                                     "from google import genai\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  3,
                      "id":  "q8EwMjpzaIDz",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "q8EwMjpzaIDz",
                                       "outputId":  "6112a173-5558-49c2-e6f9-3b56c231ec7a"
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Gemini client ready\n",
                                                       "Embedding model: gemini-embedding-001\n",
                                                       "Generation model: gemini-3-flash-preview\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Set your API key in environment variable GOOGLE_API_KEY before running.\n",
                                     "# In Colab, you can also use a secret named GOOGLE_API_KEY.\n",
                                     "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
                                     "if not api_key:\n",
                                     "    try:\n",
                                     "        from google.colab import userdata\n",
                                     "        api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
                                     "    except Exception:\n",
                                     "        api_key = None\n",
                                     "\n",
                                     "if not api_key:\n",
                                     "    raise ValueError(\"Set GOOGLE_API_KEY (environment variable or Colab secret).\")\n",
                                     "\n",
                                     "client = genai.Client(api_key=api_key)\n",
                                     "\n",
                                     "EMBED_MODEL = \"gemini-embedding-001\"\n",
                                     "GEN_MODEL = \"gemini-3-flash-preview\"\n",
                                     "\n",
                                     "print(\"Gemini client ready\")\n",
                                     "print(\"Embedding model:\", EMBED_MODEL)\n",
                                     "print(\"Generation model:\", GEN_MODEL)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  4,
                      "id":  "3Y7wj88MaIDz",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "3Y7wj88MaIDz",
                                       "outputId":  "05962416-4786-4599-f339-74e0cae8bed7"
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Downloaded documents: [\u0027gemini_cookbook\u0027, \u0027chroma_readme\u0027, \u0027langchain_readme\u0027, \u0027llamaindex_readme\u0027]\n",
                                                       "- gemini_cookbook: 14,088 chars\n",
                                                       "- chroma_readme: 5,475 chars\n",
                                                       "- langchain_readme: 6,867 chars\n",
                                                       "- llamaindex_readme: 13,291 chars\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# A small, thematically coherent corpus for RAG:\n",
                                     "# public docs/README files about Gemini, vector DBs, and RAG tooling.\n",
                                     "DOC_URLS = {\n",
                                     "    \"gemini_cookbook\": \"https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\",\n",
                                     "    \"chroma_readme\": \"https://raw.githubusercontent.com/chroma-core/chroma/main/README.md\",\n",
                                     "    \"langchain_readme\": \"https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\",\n",
                                     "    \"llamaindex_readme\": \"https://raw.githubusercontent.com/run-llama/llama_index/main/README.md\"\n",
                                     "}\n",
                                     "\n",
                                     "def fetch_text(url: str) -\u003e str:\n",
                                     "    r = requests.get(url, timeout=60)\n",
                                     "    r.raise_for_status()\n",
                                     "    return r.text\n",
                                     "\n",
                                     "raw_docs = {}\n",
                                     "for name, url in DOC_URLS.items():\n",
                                     "    raw_docs[name] = fetch_text(url)\n",
                                     "\n",
                                     "print(\"Downloaded documents:\", list(raw_docs.keys()))\n",
                                     "for k, v in raw_docs.items():\n",
                                     "    print(f\"- {k}: {len(v):,} chars\")\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  5,
                      "id":  "tDFy5OoGaIDz",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/",
                                                     "height":  161
                                                 },
                                       "id":  "tDFy5OoGaIDz",
                                       "outputId":  "7df79844-6be8-4ee6-98b1-e8cd8d619647"
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Total chunks: 40\n"
                                                   ]
                                      },
                                      {
                                          "data":  {
                                                       "text/html":  [
                                                                         "\n",
                                                                         "  \u003cdiv id=\"df-c6f0a996-c86c-4d16-927b-9b9c1361e054\" class=\"colab-df-container\"\u003e\n",
                                                                         "    \u003cdiv\u003e\n",
                                                                         "\u003cstyle scoped\u003e\n",
                                                                         "    .dataframe tbody tr th:only-of-type {\n",
                                                                         "        vertical-align: middle;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    .dataframe tbody tr th {\n",
                                                                         "        vertical-align: top;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    .dataframe thead th {\n",
                                                                         "        text-align: right;\n",
                                                                         "    }\n",
                                                                         "\u003c/style\u003e\n",
                                                                         "\u003ctable border=\"1\" class=\"dataframe\"\u003e\n",
                                                                         "  \u003cthead\u003e\n",
                                                                         "    \u003ctr style=\"text-align: right;\"\u003e\n",
                                                                         "      \u003cth\u003e\u003c/th\u003e\n",
                                                                         "      \u003cth\u003eid\u003c/th\u003e\n",
                                                                         "      \u003cth\u003edoc_id\u003c/th\u003e\n",
                                                                         "      \u003cth\u003echunk_index\u003c/th\u003e\n",
                                                                         "      \u003cth\u003etext\u003c/th\u003e\n",
                                                                         "      \u003cth\u003esource_url\u003c/th\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "  \u003c/thead\u003e\n",
                                                                         "  \u003ctbody\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e0\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003egemini_cookbook_chunk_0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003egemini_cookbook\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e0\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e# Welcome to the Gemini API Cookbook\\n\\nThis c...\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003ehttps://raw.githubusercontent.com/google-gemin...\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e1\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003egemini_cookbook_chunk_1\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003egemini_cookbook\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e1\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003eation).\\n\u0026gt; \\n\u0026gt; **ðŸŒ Nano-Banana Pro**: Go banan...\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003ehttps://raw.githubusercontent.com/google-gemin...\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "    \u003ctr\u003e\n",
                                                                         "      \u003cth\u003e2\u003c/th\u003e\n",
                                                                         "      \u003ctd\u003egemini_cookbook_chunk_2\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003egemini_cookbook\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003e2\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003ePractical use cases demonstrating how to comb...\u003c/td\u003e\n",
                                                                         "      \u003ctd\u003ehttps://raw.githubusercontent.com/google-gemin...\u003c/td\u003e\n",
                                                                         "    \u003c/tr\u003e\n",
                                                                         "  \u003c/tbody\u003e\n",
                                                                         "\u003c/table\u003e\n",
                                                                         "\u003c/div\u003e\n",
                                                                         "    \u003cdiv class=\"colab-df-buttons\"\u003e\n",
                                                                         "      \n",
                                                                         "  \u003cdiv class=\"colab-df-container\"\u003e\n",
                                                                         "    \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive(\u0027df-c6f0a996-c86c-4d16-927b-9b9c1361e054\u0027)\"\n",
                                                                         "            title=\"Convert this dataframe to an interactive table.\"\n",
                                                                         "            style=\"display:none;\"\u003e\n",
                                                                         "      \n",
                                                                         "  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\"\u003e\n",
                                                                         "    \u003cpath d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/\u003e\n",
                                                                         "  \u003c/svg\u003e\n",
                                                                         "    \u003c/button\u003e\n",
                                                                         "    \n",
                                                                         "  \u003cstyle\u003e\n",
                                                                         "    .colab-df-container {\n",
                                                                         "      display:flex;\n",
                                                                         "      gap: 12px;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    .colab-df-convert {\n",
                                                                         "      background-color: #E8F0FE;\n",
                                                                         "      border: none;\n",
                                                                         "      border-radius: 50%;\n",
                                                                         "      cursor: pointer;\n",
                                                                         "      display: none;\n",
                                                                         "      fill: #1967D2;\n",
                                                                         "      height: 32px;\n",
                                                                         "      padding: 0 0 0 0;\n",
                                                                         "      width: 32px;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    .colab-df-convert:hover {\n",
                                                                         "      background-color: #E2EBFA;\n",
                                                                         "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
                                                                         "      fill: #174EA6;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    .colab-df-buttons div {\n",
                                                                         "      margin-bottom: 4px;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    [theme=dark] .colab-df-convert {\n",
                                                                         "      background-color: #3B4455;\n",
                                                                         "      fill: #D2E3FC;\n",
                                                                         "    }\n",
                                                                         "\n",
                                                                         "    [theme=dark] .colab-df-convert:hover {\n",
                                                                         "      background-color: #434B5C;\n",
                                                                         "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
                                                                         "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
                                                                         "      fill: #FFFFFF;\n",
                                                                         "    }\n",
                                                                         "  \u003c/style\u003e\n",
                                                                         "\n",
                                                                         "    \u003cscript\u003e\n",
                                                                         "      const buttonEl =\n",
                                                                         "        document.querySelector(\u0027#df-c6f0a996-c86c-4d16-927b-9b9c1361e054 button.colab-df-convert\u0027);\n",
                                                                         "      buttonEl.style.display =\n",
                                                                         "        google.colab.kernel.accessAllowed ? \u0027block\u0027 : \u0027none\u0027;\n",
                                                                         "\n",
                                                                         "      async function convertToInteractive(key) {\n",
                                                                         "        const element = document.querySelector(\u0027#df-c6f0a996-c86c-4d16-927b-9b9c1361e054\u0027);\n",
                                                                         "        const dataTable =\n",
                                                                         "          await google.colab.kernel.invokeFunction(\u0027convertToInteractive\u0027,\n",
                                                                         "                                                    [key], {});\n",
                                                                         "        if (!dataTable) return;\n",
                                                                         "\n",
                                                                         "        const docLinkHtml = \u0027Like what you see? Visit the \u0027 +\n",
                                                                         "          \u0027\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e\u0027\n",
                                                                         "          + \u0027 to learn more about interactive tables.\u0027;\n",
                                                                         "        element.innerHTML = \u0027\u0027;\n",
                                                                         "        dataTable[\u0027output_type\u0027] = \u0027display_data\u0027;\n",
                                                                         "        await google.colab.output.renderOutput(dataTable, element);\n",
                                                                         "        const docLink = document.createElement(\u0027div\u0027);\n",
                                                                         "        docLink.innerHTML = docLinkHtml;\n",
                                                                         "        element.appendChild(docLink);\n",
                                                                         "      }\n",
                                                                         "    \u003c/script\u003e\n",
                                                                         "  \u003c/div\u003e\n",
                                                                         "  \n",
                                                                         "    \u003c/div\u003e\n",
                                                                         "  \u003c/div\u003e\n",
                                                                         "  "
                                                                     ],
                                                       "text/plain":  [
                                                                          "                        id           doc_id  chunk_index  \\\n",
                                                                          "0  gemini_cookbook_chunk_0  gemini_cookbook            0   \n",
                                                                          "1  gemini_cookbook_chunk_1  gemini_cookbook            1   \n",
                                                                          "2  gemini_cookbook_chunk_2  gemini_cookbook            2   \n",
                                                                          "\n",
                                                                          "                                                text  \\\n",
                                                                          "0  # Welcome to the Gemini API Cookbook\\n\\nThis c...   \n",
                                                                          "1  ation).\\n\u003e \\n\u003e **ðŸŒ Nano-Banana Pro**: Go banan...   \n",
                                                                          "2   Practical use cases demonstrating how to comb...   \n",
                                                                          "\n",
                                                                          "                                          source_url  \n",
                                                                          "0  https://raw.githubusercontent.com/google-gemin...  \n",
                                                                          "1  https://raw.githubusercontent.com/google-gemin...  \n",
                                                                          "2  https://raw.githubusercontent.com/google-gemin...  "
                                                                      ]
                                                   },
                                          "execution_count":  5,
                                          "metadata":  {

                                                       },
                                          "output_type":  "execute_result"
                                      }
                                  ],
                      "source":  [
                                     "def clean_text(text: str) -\u003e str:\n",
                                     "    text = text.replace(\"\\r\", \"\")\n",
                                     "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
                                     "    return text.strip()\n",
                                     "\n",
                                     "def chunk_text(text: str, chunk_size: int = 1200, overlap: int = 150):\n",
                                     "    text = clean_text(text)\n",
                                     "    chunks = []\n",
                                     "    start = 0\n",
                                     "    while start \u003c len(text):\n",
                                     "        end = min(start + chunk_size, len(text))\n",
                                     "        chunks.append(text[start:end])\n",
                                     "        if end == len(text):\n",
                                     "            break\n",
                                     "        start = end - overlap\n",
                                     "    return chunks\n",
                                     "\n",
                                     "records = []\n",
                                     "for doc_id, text in raw_docs.items():\n",
                                     "    chunks = chunk_text(text)\n",
                                     "    for i, chunk in enumerate(chunks):\n",
                                     "        records.append({\n",
                                     "            \"id\": f\"{doc_id}_chunk_{i}\",\n",
                                     "            \"doc_id\": doc_id,\n",
                                     "            \"chunk_index\": i,\n",
                                     "            \"text\": chunk,\n",
                                     "            \"source_url\": DOC_URLS[doc_id],\n",
                                     "        })\n",
                                     "\n",
                                     "df_chunks = pd.DataFrame(records)\n",
                                     "print(\"Total chunks:\", len(df_chunks))\n",
                                     "df_chunks.head(3)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  6,
                      "id":  "LayLxnRvaID0",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "LayLxnRvaID0",
                                       "outputId":  "f7c41b77-9770-4a7d-c636-fa5daca73f65"
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Embedding count: 40\n",
                                                       "Embedding dimension: 3072\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "def extract_embeddings(resp):\n",
                                     "    # Handles both single and batch embedding response shapes.\n",
                                     "    if hasattr(resp, \"embeddings\") and resp.embeddings is not None:\n",
                                     "        out = []\n",
                                     "        for emb in resp.embeddings:\n",
                                     "            if hasattr(emb, \"values\"):\n",
                                     "                out.append(list(emb.values))\n",
                                     "            elif isinstance(emb, dict) and \"values\" in emb:\n",
                                     "                out.append(list(emb[\"values\"]))\n",
                                     "        return out\n",
                                     "\n",
                                     "    if hasattr(resp, \"embedding\") and resp.embedding is not None:\n",
                                     "        emb = resp.embedding\n",
                                     "        if hasattr(emb, \"values\"):\n",
                                     "            return [list(emb.values)]\n",
                                     "        if isinstance(emb, dict) and \"values\" in emb:\n",
                                     "            return [list(emb[\"values\"])]\n",
                                     "\n",
                                     "    raise ValueError(\"Unexpected embedding response format\")\n",
                                     "\n",
                                     "texts = df_chunks[\"text\"].tolist()\n",
                                     "\n",
                                     "try:\n",
                                     "    embed_resp = client.models.embed_content(\n",
                                     "        model=EMBED_MODEL,\n",
                                     "        contents=texts,\n",
                                     "        config={\"task_type\": \"RETRIEVAL_DOCUMENT\"},\n",
                                     "    )\n",
                                     "except Exception:\n",
                                     "    embed_resp = client.models.embed_content(\n",
                                     "        model=EMBED_MODEL,\n",
                                     "        contents=texts,\n",
                                     "    )\n",
                                     "\n",
                                     "chunk_embeddings = extract_embeddings(embed_resp)\n",
                                     "\n",
                                     "print(\"Embedding count:\", len(chunk_embeddings))\n",
                                     "print(\"Embedding dimension:\", len(chunk_embeddings[0]))\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  7,
                      "id":  "ElxeM5SaaID0",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "ElxeM5SaaID0",
                                       "outputId":  "83594ba9-5836-4247-bc34-37169b9e26bf"
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Stored chunks in Chroma: 40\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# In-memory vector database (temporary).\n",
                                     "chroma_client = chromadb.Client()\n",
                                     "collection = chroma_client.get_or_create_collection(name=\"gemini_rag_demo\")\n",
                                     "\n",
                                     "collection.add(\n",
                                     "    ids=df_chunks[\"id\"].tolist(),\n",
                                     "    documents=df_chunks[\"text\"].tolist(),\n",
                                     "    embeddings=chunk_embeddings,\n",
                                     "    metadatas=[\n",
                                     "        {\n",
                                     "            \"doc_id\": row.doc_id,\n",
                                     "            \"chunk_index\": int(row.chunk_index),\n",
                                     "            \"source_url\": row.source_url,\n",
                                     "        }\n",
                                     "        for row in df_chunks.itertuples(index=False)\n",
                                     "    ],\n",
                                     ")\n",
                                     "\n",
                                     "print(\"Stored chunks in Chroma:\", collection.count())\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  8,
                      "id":  "MSm8yj-uaID0",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "MSm8yj-uaID0",
                                       "outputId":  "5f2ecf91-d636-4417-fe4c-a1c1b508f658"
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "Answer:\n",
                                                       "\n",
                                                       "The Gemini cookbook and Chroma differ in their primary functions within the AI ecosystem:\n",
                                                       "\n",
                                                       "*   **Gemini Cookbook** is a collection of guides, practical examples, and end-to-end demos focused on teaching users how to use Gemini API features, such as grounding, Batch API, and multimodal capabilities [1, 7, 8].\n",
                                                       "*   **Chroma** is a vector database used to store, embed, and query documents [3]. Its purpose is to enable \"Chat your data\" use cases by retrieving relevant document snippets based on natural language queries to provide context for an LLM [4].\n",
                                                       "\n",
                                                       "**LangChain** serves as an orchestration framework and integration layer [5]. It fits by:\n",
                                                       "*   Providing a library of integrations that connect LLMs to vector stores like Chroma [4, 5].\n",
                                                       "*   Enabling model interoperability and rapid prototyping through a modular architecture [5].\n",
                                                       "*   Offering low-level agent orchestration (via LangGraph) to build complex backend agents for applications [6, 8].\n",
                                                       "\n",
                                                       "Citation map:\n",
                                                       "[1] https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\n",
                                                       "[2] https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\n",
                                                       "[3] https://raw.githubusercontent.com/chroma-core/chroma/main/README.md\n",
                                                       "[4] https://raw.githubusercontent.com/chroma-core/chroma/main/README.md\n",
                                                       "[5] https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\n",
                                                       "[6] https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\n",
                                                       "[7] https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\n",
                                                       "[8] https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "def retrieve(query: str, k: int = 8, n_candidates: int = 24, max_per_doc: int = 2):\n",
                                     "    try:\n",
                                     "        q_resp = client.models.embed_content(\n",
                                     "            model=EMBED_MODEL,\n",
                                     "            contents=[query],\n",
                                     "            config={\"task_type\": \"RETRIEVAL_QUERY\"},\n",
                                     "        )\n",
                                     "    except Exception:\n",
                                     "        q_resp = client.models.embed_content(\n",
                                     "            model=EMBED_MODEL,\n",
                                     "            contents=[query],\n",
                                     "        )\n",
                                     "\n",
                                     "    q_emb = extract_embeddings(q_resp)[0]\n",
                                     "\n",
                                     "    res = collection.query(\n",
                                     "        query_embeddings=[q_emb],\n",
                                     "        n_results=n_candidates,\n",
                                     "    )\n",
                                     "\n",
                                     "    ranked = []\n",
                                     "    for i in range(len(res[\"ids\"][0])):\n",
                                     "        ranked.append({\n",
                                     "            \"id\": res[\"ids\"][0][i],\n",
                                     "            \"document\": res[\"documents\"][0][i],\n",
                                     "            \"metadata\": res[\"metadatas\"][0][i],\n",
                                     "            \"distance\": res[\"distances\"][0][i],\n",
                                     "        })\n",
                                     "\n",
                                     "    # Keep top relevance while preventing one source from consuming all slots.\n",
                                     "    selected = []\n",
                                     "    per_doc = {}\n",
                                     "    for r in ranked:\n",
                                     "        doc_id = r[\"metadata\"][\"doc_id\"]\n",
                                     "        count = per_doc.get(doc_id, 0)\n",
                                     "        if count \u003e= max_per_doc:\n",
                                     "            continue\n",
                                     "        selected.append(r)\n",
                                     "        per_doc[doc_id] = count + 1\n",
                                     "        if len(selected) \u003e= k:\n",
                                     "            break\n",
                                     "\n",
                                     "    # Fallback: if diversity cap is too strict, top up with best remaining chunks.\n",
                                     "    if len(selected) \u003c k:\n",
                                     "        selected_ids = {x[\"id\"] for x in selected}\n",
                                     "        for r in ranked:\n",
                                     "            if r[\"id\"] in selected_ids:\n",
                                     "                continue\n",
                                     "            selected.append(r)\n",
                                     "            if len(selected) \u003e= k:\n",
                                     "                break\n",
                                     "\n",
                                     "    return selected\n",
                                     "\n",
                                     "\n",
                                     "def answer_with_rag(question: str, k: int = 8):\n",
                                     "    hits = retrieve(question, k=k)\n",
                                     "\n",
                                     "    context_blocks = []\n",
                                     "    citations = []\n",
                                     "    for idx, h in enumerate(hits, start=1):\n",
                                     "        md = h[\"metadata\"]\n",
                                     "        context_blocks.append(\n",
                                     "            f\"[Context {idx}] source={md[\u0027doc_id\u0027]} chunk={md[\u0027chunk_index\u0027]}\\n{h[\u0027document\u0027]}\"\n",
                                     "        )\n",
                                     "        citations.append((idx, md[\"source_url\"]))\n",
                                     "\n",
                                     "    context = \"\\n\\n\".join(context_blocks)\n",
                                     "\n",
                                     "    prompt = f\"\"\"\n",
                                     "You are answering using only the provided context.\n",
                                     "If the answer is not supported by context, say: \"I don\u0027t know based on the provided documents.\"\n",
                                     "Keep the answer concise and include citation markers like [1], [2] tied to context blocks.\n",
                                     "\n",
                                     "Question:\n",
                                     "{question}\n",
                                     "\n",
                                     "Context:\n",
                                     "{context}\n",
                                     "\"\"\".strip()\n",
                                     "\n",
                                     "    resp = client.models.generate_content(\n",
                                     "        model=GEN_MODEL,\n",
                                     "        contents=prompt,\n",
                                     "    )\n",
                                     "\n",
                                     "    return {\n",
                                     "        \"question\": question,\n",
                                     "        \"answer\": resp.text,\n",
                                     "        \"hits\": hits,\n",
                                     "        \"citations\": citations,\n",
                                     "    }\n",
                                     "\n",
                                     "# Try a question.\n",
                                     "result = answer_with_rag(\"How do Gemini cookbook and Chroma differ in purpose, and where does LangChain fit?\")\n",
                                     "print(\"Answer:\\n\")\n",
                                     "print(result[\"answer\"])\n",
                                     "\n",
                                     "print(\"\\nCitation map:\")\n",
                                     "for idx, url in result[\"citations\"]:\n",
                                     "    print(f\"[{idx}] {url}\")\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  9,
                      "id":  "M0JuhnTTaID1",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "M0JuhnTTaID1",
                                       "outputId":  "74d8bb85-8253-4d07-d6f9-3981c988a886"
                                   },
                      "outputs":  [
                                      {
                                          "name":  "stdout",
                                          "output_type":  "stream",
                                          "text":  [
                                                       "================================================================================\n",
                                                       "Hit 1 | distance=0.4921 | metadata={\u0027source_url\u0027: \u0027https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\u0027, \u0027chunk_index\u0027: 5, \u0027doc_id\u0027: \u0027gemini_cookbook\u0027}\n",
                                                       "interactivity with Gemini. * **Recently Added Guides:** * [Grounding](./quickstarts/Grounding.ipynb) [![Colab](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Grounding.ipynb): Discover different ways to ground Gemini\u0027s answer using different tools, from Google Search to Youtube and URLs and the new [**Maps grounding**](https://colab.research.google.com/github/google- ...\n",
                                                       "================================================================================\n",
                                                       "Hit 2 | distance=0.4924 | metadata={\u0027source_url\u0027: \u0027https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\u0027, \u0027chunk_index\u0027: 10, \u0027doc_id\u0027: \u0027gemini_cookbook\u0027}\n",
                                                       "b) [![Colab](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Animated_Story_Video_Generation_gemini.ipynb): Create animated videos by combining Gemini\u0027s story generation, Imagen, and audio synthesis * [Plotting and mapping Live](./examples/LiveAPI_plotting_and_mapping.ipynb) [![Colab](https://storage.googleapis.com/generativeai- ...\n",
                                                       "================================================================================\n",
                                                       "Hit 3 | distance=0.5611 | metadata={\u0027doc_id\u0027: \u0027chroma_readme\u0027, \u0027chunk_index\u0027: 3, \u0027source_url\u0027: \u0027https://raw.githubusercontent.com/chroma-core/chroma/main/README.md\u0027}\n",
                                                       "dding function, or let Chroma embed them for you. 2. Query relevant documents with natural language. 3. Compose documents into the context window of an LLM like `GPT4` for additional summarization or analysis. ## Embeddings? What are embeddings? - [Read the guide from OpenAI](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) - __Literal__: Embedding something turns it from image/text/audio into a list of numbers. ðŸ–¼ï¸ or ðŸ“„ =\u003e `[1.2, 2.1, ....]`. This process makes ...\n",
                                                       "================================================================================\n",
                                                       "Hit 4 | distance=0.5896 | metadata={\u0027source_url\u0027: \u0027https://raw.githubusercontent.com/chroma-core/chroma/main/README.md\u0027, \u0027chunk_index\u0027: 2, \u0027doc_id\u0027: \u0027chroma_readme\u0027}\n",
                                                       "n these! ids=[\"doc1\", \"doc2\"], # unique for each doc ) # Query/search 2 most similar results. You can also .get by id results = collection.query( query_texts=[\"This is a query document\"], n_results=2, # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter # where_document={\"$contains\":\"search_string\"} # optional filter ) ``` Learn about all features on our [Docs](https://docs.trychroma.com) ## Features - __Simple__: Fully-typed, fully-tested, fully-documented == happiness - ...\n",
                                                       "================================================================================\n",
                                                       "Hit 5 | distance=0.6084 | metadata={\u0027source_url\u0027: \u0027https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\u0027, \u0027chunk_index\u0027: 3, \u0027doc_id\u0027: \u0027langchain_readme\u0027}\n",
                                                       "for models, embeddings, vector stores, and more. Use LangChain for: - **Real-time data augmentation**. Easily connect LLMs to diverse data sources and external/internal systems, drawing from LangChain\u0027s vast library of integrations with model providers, tools, vector stores, retrievers, and more. - **Model interoperability**. Swap models in and out as your engineering team experiments to find the best choice for your application\u0027s needs. As the industry frontier evolves, adapt quickly â€“ ...\n",
                                                       "================================================================================\n",
                                                       "Hit 6 | distance=0.6165 | metadata={\u0027chunk_index\u0027: 5, \u0027doc_id\u0027: \u0027langchain_readme\u0027, \u0027source_url\u0027: \u0027https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md\u0027}\n",
                                                       "h, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows â€“ and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab. - [Integrations](https://docs.langchain.com/oss/python/integrations/providers/overview) â€“ List of LangChain integrations, including chat \u0026 embedding models, tools \u0026 toolkits, and more - [LangSmith](https://www.langchain.com/langsmith) â€“ Helpful for agent evals and ...\n",
                                                       "================================================================================\n",
                                                       "Hit 7 | distance=0.4933 | metadata={\u0027doc_id\u0027: \u0027gemini_cookbook\u0027, \u0027chunk_index\u0027: 9, \u0027source_url\u0027: \u0027https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\u0027}\n",
                                                       "d run Python code to solve complex tasks and even output graphs * And [many more](https://github.com/google-gemini/cookbook/tree/main/quickstarts/) \u003cbr\u003e\u003cbr\u003e ## 2. Examples (Practical Use Cases) These examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications. * [Browser as a tool](./examples/Browser_as_a_tool.ipynb) [![Colab](https://storage.googleapis.com/generativeai- ...\n",
                                                       "================================================================================\n",
                                                       "Hit 8 | distance=0.5116 | metadata={\u0027source_url\u0027: \u0027https://raw.githubusercontent.com/google-gemini/cookbook/main/README.md\u0027, \u0027doc_id\u0027: \u0027gemini_cookbook\u0027, \u0027chunk_index\u0027: 11}\n",
                                                       "o.py): Use gradio to deploy your own instance of the *Live API* * And [many many more](https://github.com/google-gemini/cookbook/tree/main/examples/) \u003cbr\u003e\u003cbr\u003e ## 3. Demos (End-to-End Applications) These fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios. * [Gemini CLI](https://github.com/google-gemini/gemini-cli): Open-source AI agent that brings the power of Gemini directly into your terminal * [Gemini API quickstart](https://github.com/google- ...\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Inspect retrieved chunks for debugging / teaching.\n",
                                     "for i, h in enumerate(result[\"hits\"], start=1):\n",
                                     "    print(\"=\" * 80)\n",
                                     "    print(f\"Hit {i} | distance={h[\u0027distance\u0027]:.4f} | metadata={h[\u0027metadata\u0027]}\")\n",
                                     "    print(textwrap.shorten(h[\"document\"].replace(\"\\n\", \" \"), width=500, placeholder=\" ...\"))\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "Jit8dCeIaID1",
                      "metadata":  {
                                       "id":  "Jit8dCeIaID1"
                                   },
                      "source":  [
                                     "## Exercises\n",
                                     "1. Change `DOC_URLS` to your own domain corpus (policies, manuals, course notes).\n",
                                     "2. Increase `k` in retrieval and compare answer quality vs. verbosity.\n",
                                     "3. Add a reranking step (optional) before generation.\n",
                                     "4. Add citation post-checking: verify each claim appears in retrieved chunks.\n"
                                 ]
                  }
              ],
    "metadata":  {
                     "colab":  {
                                   "provenance":  [

                                                  ]
                               },
                     "kernelspec":  {
                                        "display_name":  "Python 3 (ipykernel)",
                                        "language":  "python",
                                        "name":  "python3"
                                    }
                 },
    "nbformat":  4,
    "nbformat_minor":  5
}
