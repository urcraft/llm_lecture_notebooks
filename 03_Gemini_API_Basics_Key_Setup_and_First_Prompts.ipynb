{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR0x9Lff8D4X"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/urcraft/llm_lecture_notebooks/blob/main/03_Gemini_API_Basics_Key_Setup_and_First_Prompts.ipynb\">   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a>"
      ],
      "id": "JR0x9Lff8D4X"
    },
    {
      "cell_type": "markdown",
      "id": "SSdMAJbEN8dX",
      "metadata": {
        "id": "SSdMAJbEN8dX"
      },
      "source": [
        "# Gemini API Basics: Free Key, First Calls, and Prompting\n",
        "\n",
        "## What you will learn\n",
        "- What a closed-model API is and why teams use it.\n",
        "- How to use a free Gemini API key in Colab.\n",
        "- How prompt changes affect model outputs.\n",
        "\n",
        "Expected runtime: 30-45 minutes\n",
        "Expected cost: Free tier if usage stays within quota.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qvdmzvtdN8dZ",
      "metadata": {
        "id": "qvdmzvtdN8dZ"
      },
      "source": [
        "## Setup notes\n",
        "1. Store your key as a Colab Secret named GOOGLE_API_KEY.\n",
        "2. If key setup fails, you can still complete analysis tasks using reference outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "TTKeMeCQN8dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTKeMeCQN8dc",
        "outputId": "1fb76236-0224-43ba-8911-c112c35dcdfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/728.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m727.0/728.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.8/728.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip -q install -U google-genai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "hTrJhCHiN8dd",
      "metadata": {
        "id": "hTrJhCHiN8dd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "GEMINI_AVAILABLE = False\n",
        "GEMINI_ERROR = None\n",
        "\n",
        "try:\n",
        "    from google import genai\n",
        "\n",
        "    api_key = os.getenv('GOOGLE_API_KEY')\n",
        "    if not api_key:\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            api_key = userdata.get('GOOGLE_API_KEY')\n",
        "        except Exception:\n",
        "            api_key = None\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError('GOOGLE_API_KEY not found. Set env var or Colab secret GOOGLE_API_KEY.')\n",
        "\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    GEMINI_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    GEMINI_ERROR = str(e)\n",
        "    print('Gemini is not available in this runtime.')\n",
        "    print('Reason:', GEMINI_ERROR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "-kDyJwMmN8de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kDyJwMmN8de",
        "outputId": "92f9db4e-0a9f-49ee-d3ae-7feab588c203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: gemini-2.5-flash\n"
          ]
        }
      ],
      "source": [
        "MODEL_ID = 'gemini-2.5-flash'\n",
        "print('Model:', MODEL_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "JR4hhwv2N8df",
      "metadata": {
        "id": "JR4hhwv2N8df"
      },
      "outputs": [],
      "source": [
        "def run_gemini(prompt: str, system_instruction=None):\n",
        "    if not GEMINI_AVAILABLE:\n",
        "        return {\n",
        "            'ok': False,\n",
        "            'model': MODEL_ID,\n",
        "            'prompt': prompt,\n",
        "            'system_instruction': system_instruction,\n",
        "            'text': None,\n",
        "            'latency_s': None,\n",
        "            'tokens_in': None,\n",
        "            'error': GEMINI_ERROR,\n",
        "        }\n",
        "\n",
        "    config = {'system_instruction': system_instruction} if system_instruction else None\n",
        "\n",
        "    try:\n",
        "        start = time.perf_counter()\n",
        "        response = client.models.generate_content(model=MODEL_ID, contents=prompt, config=config)\n",
        "        latency = time.perf_counter() - start\n",
        "\n",
        "        token_info = client.models.count_tokens(model=MODEL_ID, contents=prompt)\n",
        "        tokens_in = getattr(token_info, 'total_tokens', None)\n",
        "\n",
        "        return {\n",
        "            'ok': True,\n",
        "            'model': MODEL_ID,\n",
        "            'prompt': prompt,\n",
        "            'system_instruction': system_instruction,\n",
        "            'text': response.text,\n",
        "            'latency_s': round(latency, 2),\n",
        "            'tokens_in': tokens_in,\n",
        "            'error': None,\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'ok': False,\n",
        "            'model': MODEL_ID,\n",
        "            'prompt': prompt,\n",
        "            'system_instruction': system_instruction,\n",
        "            'text': None,\n",
        "            'latency_s': None,\n",
        "            'tokens_in': None,\n",
        "            'error': str(e),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1a219189",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a219189",
        "outputId": "d977054e-90c7-47f1-ef3e-00d77dea4393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- First Gemini Call ---\n",
            "Prompt: Explain GPU and VRAM to a high-school student in 5 bullet points.\n",
            "Error: None\n",
            "Output:\n",
            "\n",
            "Here's an explanation of GPU and VRAM in 5 bullet points:\n",
            "\n",
            "*   **GPU (Graphics Processing Unit): The Visual Artist:** Think of the GPU as a highly specialized super-artist inside your computer. While your main processor (CPU) is a general-purpose genius handling everything from spreadsheets to web browsing, the GPU is *specifically* designed to quickly render all the images, videos, and 3D graphics you see on your screen. It does this by performing thousands of simple calculations at the same time, perfect for creating detailed game worlds or editing high-resolution video.\n",
            "\n",
            "*   **GPU vs. CPU: Different Strengths:** Imagine the CPU as a master chef who can cook any dish perfectly, but only one at a time. The GPU is like a whole team of sous chefs, each capable of doing a small, specific task very quickly (like chopping onions), allowing them to prepare huge meals much faster by working in parallel. This parallel processing power is why GPUs excel at graphics and tasks like AI, which involve many repetitive calculations.\n",
            "\n",
            "*   **VRAM (Video Random Access Memory): The Artist's Workspace:** VRAM is a special type of super-fast memory that lives directly on the graphics card, right next to the GPU. Think of it as the GPU's dedicated \"workbench\" or \"sketchpad.\" Instead of the GPU having to constantly fetch visual data (like textures for a game character, 3D models, or the next frame of a video) from the slower main system RAM, VRAM stores all that crucial visual information for instant access.\n",
            "\n",
            "*   **How They Work Together:** When you're playing a game, the GPU is constantly drawing new frames, and it needs immediate access to all the visual assets. VRAM holds these assets – like the detailed skin texture of your character, the geometry of a building, or the current state of the game world. The GPU fetches this data from VRAM almost instantaneously, processes it, and then writes the final image back into VRAM, ready to be displayed on your screen.\n",
            "\n",
            "*   **Why They Matter to You:** A powerful GPU and ample VRAM are crucial for smooth performance in visually demanding tasks. More powerful GPUs can render more complex graphics faster, leading to higher frame rates in games or quicker video exports. More VRAM allows the GPU to store more detailed textures, larger 3D models, and more complex scenes, preventing \"stuttering\" or lag, especially at higher resolutions or graphic settings.\n",
            "\n",
            "--- System Prompt Comparison Setup ---\n",
            "User prompt: Explain the concept of large language models (LLMs).\n",
            "System Prompt 1: You are a helpful assistant that provides concise answers.\n",
            "System Prompt 2: You are a helpful assistant that provides detailed explanations, especially for technical terms.\n"
          ]
        }
      ],
      "source": [
        "# First basic call (no system prompt)\n",
        "first_prompt = 'Explain GPU and VRAM to a high-school student in 5 bullet points.'\n",
        "first_result = run_gemini(first_prompt)\n",
        "\n",
        "print('--- First Gemini Call ---')\n",
        "print('Prompt:', first_prompt)\n",
        "print('Error:', first_result['error'])\n",
        "print('Output:\\n')\n",
        "print(first_result['text'])\n",
        "\n",
        "# Prompt for system prompt comparison\n",
        "comparison_prompt = 'Explain the concept of large language models (LLMs).'\n",
        "system_prompt_1 = 'You are a helpful assistant that provides concise answers.'\n",
        "system_prompt_2 = 'You are a helpful assistant that provides detailed explanations, especially for technical terms.'\n",
        "\n",
        "print('\\n--- System Prompt Comparison Setup ---')\n",
        "print(f'User prompt: {comparison_prompt}')\n",
        "print(f'System Prompt 1: {system_prompt_1}')\n",
        "print(f'System Prompt 2: {system_prompt_2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "18f120de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "18f120de",
        "outputId": "bb2ce7f7-b978-426a-8b2b-545a7e72936a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Output: No System Prompt ---\n",
            "Large Language Models (LLMs) are a type of artificial intelligence (AI) program designed to understand, generate, and manipulate human language. They are at the forefront of the current AI revolution due to their remarkable capabilities in various text-based tasks.\n",
            "\n",
            "Let's break down the concept:\n",
            "\n",
            "### What's in the Name?\n",
            "\n",
            "1.  **Language:**\n",
            "    *   LLMs are specifically trained on vast amounts of text data (books, articles, websites, code, conversations, etc.).\n",
            "    *   Their primary function is to process and produce natural language, meaning the way humans communicate.\n",
            "    *   They learn the nuances of grammar, syntax, semantics, context, style, and even tone.\n",
            "\n",
            "2.  **Model:**\n",
            "    *   In AI, a \"model\" is a trained algorithm. It's a mathematical structure that has learned patterns and relationships from data.\n",
            "    *   LLMs are typically built using a **neural network architecture**, specifically a type called **Transformers**. Transformers are highly effective at processing sequential data like text because they can weigh the importance of different words in a sequence (this is called the \"attention mechanism\").\n",
            "    *   The \"model\" is essentially a complex set of weights and biases (parameters) that determine how it processes input and generates output.\n",
            "\n",
            "3.  **Large:** This is a crucial distinguishing factor:\n",
            "    *   **Parameters:** LLMs have billions, even trillions, of parameters. These parameters represent the learned knowledge and relationships within the model. More parameters generally allow the model to learn more complex patterns and store more \"information.\"\n",
            "    *   **Training Data:** They are trained on truly colossal datasets, often comprising a significant portion of all publicly available text on the internet. This massive exposure allows them to learn a wide range of topics, writing styles, factual information, and linguistic patterns.\n",
            "    *   **Computational Resources:** Training and running these models require immense computational power (GPUs, specialized hardware) and energy.\n",
            "\n",
            "### How Do They Work (Simplified)?\n",
            "\n",
            "At their core, LLMs are **next-word prediction machines**. Here's the general process:\n",
            "\n",
            "1.  **Training:**\n",
            "    *   During training, the model is fed billions of words from its vast dataset.\n",
            "    *   Its primary task is to predict the next word in a given sequence. For example, if it sees \"The cat sat on the...\", it learns that \"mat,\" \"couch,\" or \"floor\" are highly probable next words.\n",
            "    *   Through repeated predictions and adjustments (using complex mathematical optimization algorithms), the model learns the statistical relationships between words, phrases, and sentences. It develops an internal representation of language, including grammar rules, factual knowledge, and common sense reasoning implied by the text.\n",
            "\n",
            "2.  **Inference (Generation):**\n",
            "    *   When you give an LLM a \"prompt\" (your input text, e.g., \"Write a poem about a sunrise\"), the model takes that prompt as its starting sequence.\n",
            "    *   It then predicts the most probable next word based on its training.\n",
            "    *   Once it generates that word, it adds it to the sequence and predicts the *next* most probable word, and so on.\n",
            "    *   This iterative process of predicting one word at a time, building upon the previous output, allows it to generate coherent, contextually relevant, and often surprisingly creative long-form text.\n",
            "    *   It's not just picking the *single* most probable word; it samples from a probability distribution, which allows for variability and creativity in its responses (controlled by parameters like \"temperature\").\n",
            "\n",
            "### Key Capabilities and Characteristics:\n",
            "\n",
            "*   **Text Generation:** Writing articles, stories, poems, code, emails, marketing copy.\n",
            "*   **Question Answering:** Providing information, summarizing facts, explaining concepts.\n",
            "*   **Translation:** Translating text from one human language to another.\n",
            "*   **Summarization:** Condensing long documents into shorter versions.\n",
            "*   **Code Generation & Explanation:** Writing code in various programming languages, debugging, and explaining complex code.\n",
            "*   **Conversational AI:** Engaging in human-like dialogue, acting as chatbots or virtual assistants.\n",
            "*   **Reasoning (Emergent):** While not explicitly programmed for reasoning, their massive training enables them to exhibit impressive abilities in logical inference, problem-solving, and analogy.\n",
            "*   **Context Understanding:** They can maintain context over long conversations or documents, referring back to previous parts of the input.\n",
            "*   **Few-Shot/Zero-Shot Learning:** They can perform new tasks with very few or even no examples, simply by being prompted correctly (e.g., \"Translate this English sentence into French: [sentence]\").\n",
            "\n",
            "### Limitations and Challenges:\n",
            "\n",
            "*   **Hallucinations:** LLMs can generate plausible-sounding but factually incorrect information with high confidence. They don't \"know\" facts in the human sense but generate text that *looks* like a fact.\n",
            "*   **Bias:** They inherit biases present in their training data. If the internet contains biases (which it does), the LLM can reproduce or even amplify them.\n",
            "*   **Lack of True Understanding:** LLMs don't possess consciousness, emotions, or genuine understanding of the world. They are sophisticated pattern-matching systems.\n",
            "*   **Knowledge Cut-off:** Their knowledge is limited by their training data. They don't inherently have real-time information unless specifically integrated with search tools.\n",
            "*   **Computational Cost:** Training and running these models are very expensive.\n",
            "*   **Ethical Concerns:** Misinformation, misuse for malicious purposes, job displacement, copyright issues with training data.\n",
            "\n",
            "### Examples of LLMs:\n",
            "\n",
            "*   **GPT Series** (OpenAI: GPT-3, GPT-3.5, GPT-4)\n",
            "*   **Llama Series** (Meta)\n",
            "*   **Gemini** (Google)\n",
            "*   **Claude** (Anthropic)\n",
            "*   **Falcon** (Technology Innovation Institute)\n",
            "\n",
            "In essence, LLMs are incredibly powerful and versatile AI systems that have revolutionized how we interact with information and create content, leveraging massive datasets and sophisticated neural network architectures to simulate human-like language capabilities.\n",
            "\n",
            "--- Output: System Prompt 1 (Concise) ---\n",
            "Large Language Models (LLMs) are deep learning AI models trained on vast amounts of text data to understand, generate, and process human language.\n",
            "\n",
            "Key characteristics:\n",
            "\n",
            "*   **Scale:** They have billions or even trillions of parameters and are trained on massive datasets (web pages, books, articles).\n",
            "*   **Architecture:** Most use the transformer architecture, enabling them to process sequences of text and understand context.\n",
            "*   **Training:** They learn patterns and relationships in language through self-supervised training, often by predicting the next word in a sentence.\n",
            "*   **Capabilities:** They can perform a wide range of tasks, including answering questions, writing essays, summarizing text, translating languages, and generating creative content.\n",
            "\n",
            "--- Output: System Prompt 2 (Detailed) ---\n",
            "Large Language Models (LLMs) are a type of artificial intelligence (AI) model that has been trained on an enormous amount of text data to understand, generate, and process human language with remarkable fluency and coherence. They represent a significant leap forward in natural language processing (NLP) and are the technology behind popular AI tools like ChatGPT, Google Bard (now Gemini), and Microsoft Copilot.\n",
            "\n",
            "Let's break down the core concepts:\n",
            "\n",
            "---\n",
            "\n",
            "### What Does \"Large Language Model\" Mean?\n",
            "\n",
            "1.  **Large:** This refers to several aspects:\n",
            "    *   **Training Data:** LLMs are trained on truly massive datasets, often comprising trillions of \"tokens\" (words or sub-word units) pulled from the internet (web pages, books, articles, code, social media, etc.). This vast exposure allows them to learn an incredible breadth of knowledge, grammar, linguistic patterns, and even some forms of reasoning.\n",
            "    *   **Model Size (Parameters):** LLMs have billions, even hundreds of billions, of \"parameters.\" Parameters are the numerical values that the model learns during training and essentially define its knowledge and capabilities. More parameters generally mean a greater capacity to learn complex patterns and store information. For example, GPT-3 has 175 billion parameters.\n",
            "    *   **Computational Power:** Training and running these models require immense computational resources (high-performance GPUs or TPUs) and energy.\n",
            "\n",
            "2.  **Language:** This indicates their primary domain and function:\n",
            "    *   **Human Language Understanding:** They can interpret the meaning, context, and intent behind human language inputs (prompts).\n",
            "    *   **Human Language Generation:** They can produce new, coherent, and contextually relevant text that often mimics human writing style across various genres and tasks.\n",
            "\n",
            "3.  **Model:** This signifies that they are a mathematical construct or algorithm:\n",
            "    *   **Statistical Prediction Machine:** At their core, LLMs are complex statistical models designed to predict the most probable next word (or sequence of words) given a preceding context. They don't \"understand\" in a human sense but rather learn patterns and relationships from their training data.\n",
            "    *   **Deep Learning Architecture:** They typically use a deep learning architecture called a **Transformer**, which is particularly effective at handling sequential data like text.\n",
            "\n",
            "---\n",
            "\n",
            "### How Do LLMs Work? (The Technical Details)\n",
            "\n",
            "The process of creating and using an LLM can be simplified into these key stages:\n",
            "\n",
            "1.  **Architecture: The Transformer**\n",
            "    *   The breakthrough that enabled LLMs was the **Transformer architecture**, introduced by Google in 2017. Before Transformers, models like Recurrent Neural Networks (RNNs) struggled with long-range dependencies in text.\n",
            "    *   **Self-Attention Mechanism:** The core innovation of the Transformer is the \"self-attention\" mechanism. This allows the model to weigh the importance of different words in an input sequence relative to each other, regardless of their position. For example, in the sentence \"The cat, which was small, chased the mouse,\" the model can directly relate \"cat\" to \"chased\" and \"mouse,\" even with the intervening phrase. This is crucial for understanding context and relationships over long texts.\n",
            "    *   Transformers process text in parallel, making them much faster and more scalable for huge datasets than previous architectures.\n",
            "\n",
            "2.  **Pre-training:**\n",
            "    *   This is the most computationally intensive phase. The model is fed vast amounts of raw text data.\n",
            "    *   **Objective:** The primary pre-training task is typically **next-token prediction** (also known as \"causal language modeling\"). Given a sequence of words, the model tries to predict the next word. By repeatedly performing this task across trillions of examples, the LLM learns:\n",
            "        *   **Grammar and Syntax:** How language is structured.\n",
            "        *   **Facts and World Knowledge:** Information embedded within the text.\n",
            "        *   **Reasoning Patterns:** Implicit relationships and common sense derived from how information is presented.\n",
            "        *   **Styles and Tones:** Different ways people write.\n",
            "\n",
            "3.  **Fine-tuning (and Instruction Tuning/RLHF):**\n",
            "    *   After pre-training, the LLM has a broad understanding of language but might not be good at following specific instructions or having helpful conversations. This is where fine-tuning comes in.\n",
            "    *   **Instruction Tuning:** The model is further trained on datasets of prompt-response pairs, where humans (or other models) have curated examples of instructions and ideal answers. This teaches the model to understand prompts, follow specific commands, and format its output appropriately.\n",
            "    *   **Reinforcement Learning from Human Feedback (RLHF):** This is a critical step for making LLMs truly conversational and aligned with human values.\n",
            "        *   Humans rank or score different model responses to a prompt based on helpfulness, harmlessness, and accuracy.\n",
            "        *   This feedback is used to train a \"reward model.\"\n",
            "        *   The LLM is then further refined using reinforcement learning to generate responses that are likely to receive high scores from the reward model. This process significantly improves the model's ability to engage in natural, safe, and useful dialogue.\n",
            "\n",
            "---\n",
            "\n",
            "### Key Characteristics & Capabilities of LLMs:\n",
            "\n",
            "*   **Generative:** They can create new, original text (articles, stories, code, emails, poems, etc.).\n",
            "*   **Contextual Understanding:** They can maintain context over long conversations or documents, thanks to the Transformer's self-attention.\n",
            "*   **Emergent Abilities:** As models scale, they often exhibit \"emergent abilities\" – capabilities they weren't explicitly trained for but arise from their vast knowledge and pattern recognition (e.g., complex reasoning, code generation, multi-step problem-solving).\n",
            "*   **Multilingual:** Many LLMs are trained on multilingual data and can understand and generate text in multiple languages.\n",
            "*   **Versatile:** They can perform a wide range of NLP tasks:\n",
            "    *   **Question Answering:** Providing information based on their training data.\n",
            "    *   **Summarization:** Condensing long texts into shorter versions.\n",
            "    *   **Translation:** Converting text between languages.\n",
            "    *   **Code Generation:** Writing, debugging, or explaining programming code.\n",
            "    *   **Content Creation:** Drafting marketing copy, scripts, social media posts.\n",
            "    *   **Brainstorming and Ideation:** Generating creative ideas or solutions.\n",
            "    *   **Conversational AI:** Acting as chatbots or virtual assistants.\n",
            "\n",
            "---\n",
            "\n",
            "### Limitations and Challenges:\n",
            "\n",
            "*   **Hallucinations:** LLMs can confidently generate incorrect, nonsensical, or made-up information because they predict the most *probable* next word, not necessarily the *factually correct* one. They don't \"know\" truth in a human sense.\n",
            "*   **Bias:** They can inherit and amplify biases present in their vast training data, leading to unfair, prejudiced, or stereotypical outputs.\n",
            "*   **Lack of Real-World Understanding:** LLMs lack genuine common sense, emotional intelligence, or understanding of the physical world. They operate purely on linguistic patterns.\n",
            "*   **Opacity (Black Box):** It's often difficult to understand *why* an LLM produced a particular output, making debugging and ensuring reliability challenging.\n",
            "*   **Computational Cost:** Training and deploying LLMs are extremely expensive in terms of computing power and energy.\n",
            "*   **Ethical Concerns:** Misinformation, misuse for malicious purposes (e.g., generating propaganda), copyright issues, and job displacement are significant concerns.\n",
            "*   **Factuality:** While they can regurgitate facts learned during training, they are not always reliable as factual sources and can become outdated quickly.\n",
            "\n",
            "---\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "Large Language Models are powerful tools that have revolutionized how we interact with information and AI. By learning intricate patterns from vast amounts of human text, they can perform an impressive array of language-related tasks. However, it's crucial to understand their probabilistic nature and inherent limitations to use them effectively and responsibly. The field is rapidly evolving, with ongoing research focusing on making LLMs more reliable, ethical, and efficient.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 prompt_type  latency_s  tokens_in error  \\\n",
              "0           No System Prompt      11.97         12  None   \n",
              "1   System Prompt 1: Concise       1.76         12  None   \n",
              "2  System Prompt 2: Detailed      14.37         12  None   \n",
              "\n",
              "                                                text  \n",
              "0  Large Language Models (LLMs) are a type of art...  \n",
              "1  Large Language Models (LLMs) are deep learning...  \n",
              "2  Large Language Models (LLMs) are a type of art...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab2db33b-fb81-488d-b8c2-b474b2fe8f20\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt_type</th>\n",
              "      <th>latency_s</th>\n",
              "      <th>tokens_in</th>\n",
              "      <th>error</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No System Prompt</td>\n",
              "      <td>11.97</td>\n",
              "      <td>12</td>\n",
              "      <td>None</td>\n",
              "      <td>Large Language Models (LLMs) are a type of art...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>System Prompt 1: Concise</td>\n",
              "      <td>1.76</td>\n",
              "      <td>12</td>\n",
              "      <td>None</td>\n",
              "      <td>Large Language Models (LLMs) are deep learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>System Prompt 2: Detailed</td>\n",
              "      <td>14.37</td>\n",
              "      <td>12</td>\n",
              "      <td>None</td>\n",
              "      <td>Large Language Models (LLMs) are a type of art...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab2db33b-fb81-488d-b8c2-b474b2fe8f20')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ab2db33b-fb81-488d-b8c2-b474b2fe8f20 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ab2db33b-fb81-488d-b8c2-b474b2fe8f20');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_31c6caf5-28a9-453c-8562-4bacf807720d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('comparison_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_31c6caf5-28a9-453c-8562-4bacf807720d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('comparison_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "comparison_df",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "baseline_result = run_gemini(comparison_prompt)\n",
        "result_sp1 = run_gemini(comparison_prompt, system_instruction=system_prompt_1)\n",
        "result_sp2 = run_gemini(comparison_prompt, system_instruction=system_prompt_2)\n",
        "\n",
        "print('\\n--- Output: No System Prompt ---')\n",
        "print(baseline_result['text'])\n",
        "\n",
        "print('\\n--- Output: System Prompt 1 (Concise) ---')\n",
        "print(result_sp1['text'])\n",
        "\n",
        "print('\\n--- Output: System Prompt 2 (Detailed) ---')\n",
        "print(result_sp2['text'])\n",
        "\n",
        "comparison_df = pd.DataFrame([\n",
        "    {'prompt_type': 'No System Prompt', 'latency_s': baseline_result['latency_s'], 'tokens_in': baseline_result['tokens_in'], 'error': baseline_result['error'], 'text': baseline_result['text']},\n",
        "    {'prompt_type': 'System Prompt 1: Concise', 'latency_s': result_sp1['latency_s'], 'tokens_in': result_sp1['tokens_in'], 'error': result_sp1['error'], 'text': result_sp1['text']},\n",
        "    {'prompt_type': 'System Prompt 2: Detailed', 'latency_s': result_sp2['latency_s'], 'tokens_in': result_sp2['tokens_in'], 'error': result_sp2['error'], 'text': result_sp2['text']}\n",
        "])\n",
        "\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "jUtCZuERN8df",
      "metadata": {
        "id": "jUtCZuERN8df",
        "outputId": "86c679a8-0c9c-45ec-b97c-7255bb72b716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              prompt  latency_s  tokens_in  \\\n",
              "0  What is the difference between open-weight and...       7.19         17   \n",
              "1  Compare open-weight and closed models for a sm...      17.24         24   \n",
              "\n",
              "  error                                               text  \n",
              "0  None  The core difference is about **who gets the \"b...  \n",
              "1  None  For a small startup, the choice between open-w...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-241ed2a7-ecf5-449b-9501-d9cd4a65f706\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>latency_s</th>\n",
              "      <th>tokens_in</th>\n",
              "      <th>error</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the difference between open-weight and...</td>\n",
              "      <td>7.19</td>\n",
              "      <td>17</td>\n",
              "      <td>None</td>\n",
              "      <td>The core difference is about **who gets the \"b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Compare open-weight and closed models for a sm...</td>\n",
              "      <td>17.24</td>\n",
              "      <td>24</td>\n",
              "      <td>None</td>\n",
              "      <td>For a small startup, the choice between open-w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-241ed2a7-ecf5-449b-9501-d9cd4a65f706')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-241ed2a7-ecf5-449b-9501-d9cd4a65f706 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-241ed2a7-ecf5-449b-9501-d9cd4a65f706');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Additional prompt examples for basic API calling\n",
        "prompts = [\n",
        "    'What is the difference between open-weight and closed models? Keep it simple.',\n",
        "    'Compare open-weight and closed models for a small startup. Include privacy, cost, and control in a table.'\n",
        "]\n",
        "\n",
        "rows = [run_gemini(p) for p in prompts]\n",
        "results_df = pd.DataFrame(rows)\n",
        "results_df[['prompt', 'latency_s', 'tokens_in', 'error', 'text']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5UKBi0jpN8df",
      "metadata": {
        "id": "5UKBi0jpN8df",
        "outputId": "44bae887-f179-4c23-c37d-4fca3a7796a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 prompt_type  latency_s  tokens_in error  \\\n",
              "0           No System Prompt      11.97         12  None   \n",
              "1   System Prompt 1: Concise       1.76         12  None   \n",
              "2  System Prompt 2: Detailed      14.37         12  None   \n",
              "\n",
              "                                        text_preview  \n",
              "0  Large Language Models (LLMs) are a type of art...  \n",
              "1  Large Language Models (LLMs) are deep learning...  \n",
              "2  Large Language Models (LLMs) are a type of art...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0bc6abe5-ec67-4576-8082-44986d06266a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt_type</th>\n",
              "      <th>latency_s</th>\n",
              "      <th>tokens_in</th>\n",
              "      <th>error</th>\n",
              "      <th>text_preview</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No System Prompt</td>\n",
              "      <td>11.97</td>\n",
              "      <td>12</td>\n",
              "      <td>None</td>\n",
              "      <td>Large Language Models (LLMs) are a type of art...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>System Prompt 1: Concise</td>\n",
              "      <td>1.76</td>\n",
              "      <td>12</td>\n",
              "      <td>None</td>\n",
              "      <td>Large Language Models (LLMs) are deep learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>System Prompt 2: Detailed</td>\n",
              "      <td>14.37</td>\n",
              "      <td>12</td>\n",
              "      <td>None</td>\n",
              "      <td>Large Language Models (LLMs) are a type of art...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0bc6abe5-ec67-4576-8082-44986d06266a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0bc6abe5-ec67-4576-8082-44986d06266a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0bc6abe5-ec67-4576-8082-44986d06266a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Optional: quick side-by-side excerpt view for teaching\n",
        "preview_df = comparison_df.copy()\n",
        "preview_df['text_preview'] = preview_df['text'].fillna('').str.slice(0, 240)\n",
        "preview_df[['prompt_type', 'latency_s', 'tokens_in', 'error', 'text_preview']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NpjJOIQAN8dg",
      "metadata": {
        "id": "NpjJOIQAN8dg"
      },
      "source": [
        "## Checkpoint\n",
        "- Rewrite a weak prompt into a strong prompt for the same task.\n",
        "- Note 2 output differences you observe.\n",
        "\n",
        "## Reflection\n",
        "- When would a closed API model be better than running local models?\n",
        "\n",
        "## Troubleshooting\n",
        "- If key lookup fails, re-open Colab Secrets and verify GOOGLE_API_KEY.\n",
        "- Free-tier quota limits can cause temporary request failures.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}